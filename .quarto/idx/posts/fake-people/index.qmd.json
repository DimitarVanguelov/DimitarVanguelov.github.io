{"title":"Generating 1 Billion Fake People with Julia","markdown":{"yaml":{"title":"Generating 1 Billion Fake People with Julia","date":"2023-02-27"},"headingText":"The Details","containsRefs":false,"markdown":"\n\nIn a [recent MotherDuck blog post](https://motherduck.com/blog/python-faker-duckdb-exploration/), \nthe author generated 1 billion fake people records using Python in order to analyze the data \nwith DuckDB. I suspect the point of the article was to showcase how awesome `duckdb` is at \nhandling large amounts of local data, but it did spend the majority of its time explaining the \ndata generation process, which made for a fun read.\n\nOne of the more interesting tidbits from the article was:\n\n> I used the GNU Parallel technique discussed above with a hefty m6i.32xlarge instance on \nAmazon EC2, though generated a billion people in 1k parquet files. This took about 2 hours to \ngenerate.\n\nYikes, that's a lot of firepower! [That machine]((https://instances.vantage.sh/aws/ec2/m6i.32xlarge)) \ncomes with 128 vCPUs and 512 GiB RAM, and costs about $6 an hour. So pretty hefty indeed.\n\nBeing a big fan of Julia, I decided to see what it would be like to generate such a dataset \nwith Julia. More concretely, I wanted to see if I can use less resources (only my laptop) *and* \nhave the process run in significantly less time. Not only that, I thought it be fun to explore \nthe various modes of concurrency (multi-threading, multi-processing, async, etc) Julia offers.\n\nThe results were initially disappointing, as I'll explain below, but in the end I did get that \nnice speed-up I was looking for.\n\n\n\nThe [original post](https://motherduck.com/blog/python-faker-duckdb-exploration/) has the full \ndetails, but I'll go over the basic details here. A `person` record consists of the following \nrandomly generated fields:\n```\n- id\n- first_name\n- last_name\n- email\n- company\n- phone\n```\n\nUsing the Python [Faker](https://faker.readthedocs.io/en/master/) library to generate the \ndata and [GNU Parallel](https://www.gnu.org/software/parallel/) to parallelize the operation, \nthe author created 1,000 parquet files with 1 million records each before populating a \n`duckdb` database for further analysis.\n\n\n## Julia: First Attempt\n\nLuckily, Julia has its own [Faker.jl](https://github.com/neomatrixcode/Faker.jl) package. \nUsing it is as simple as:\n\n```julia\nusing Faker\n\nFaker.first_name()\n```\n\n```\n\"Wilfredo\"\n```\n\nInstead of putting all the fields in a dictionary, I created a struct instead:\n\n```julia\nstruct Person\n    id::String\n    first_name::String\n    last_name::String\n    email::String\n    company::String\n    phone::String\nend\n```\n\nAside from being a natural thing to do in Julia, this ended up being a really handy vehicle \nfor populating a DataFrame, as we'll see in a moment.\n\nIn order to construct the `Person` object, we have the following function, which is essentially \nthe same as in the Python version in the original post:\n\n```julia\nfunction get_person()\n    person = Person(\n        Faker.random_int(min=1000, max=9999999999999),\n        Faker.first_name(),\n        Faker.last_name(),\n        Faker.email(),\n        Faker.company(),\n        Faker.phone_number()\n    )\n    return person\nend\n\nget_person()\n```\n\n```\nPerson(\"8429894898777\", \"Christin\", \"Gleason\", \"Archie99@yahoo.com\", \"Mante, Hilll and Hessel\", \"1-548-869-5799 x26945\")\n```\n\n\nThis approach clearly suffers from the same deficiency as the original in that the generated \nemail address bears absolutely no semblance to the generated first and last names. But \nthat's ok, we're just making up data for mocking and testing purposes anyhow.\n\nTo create an array of `Person`s, we can use a comprehension:\n\n```julia\nlist_of_five = [get_person() for _ in 1:5]\n```\n\n```\n5-element Vector{Person}:\n Person(\"502327436522\", \"Simon\", \"Lind\", \"Franklyn.Satterfield@yahoo.com\", \"Rutherford-Barton\", \"054.718.0236\")\n Person(\"1988647737198\", \"Charlott\", \"Jacobs\", \"Walter.Ziemann@hotmail.com\", \"Towne, Gorczany and Brekke\", \"839-605-0245 x477\")\n Person(\"3335059941285\", \"Glory\", \"Brakus\", \"Nienow.Cassandra@lh.net\", \"Schuppe, Powlowski and Powlowski\", \"(122) 872-3081 x3643\")\n Person(\"4996530776723\", \"Hedwig\", \"Pfannerstill\", \"wSchamberger@hg.net\", \"Langosh Group\", \"(594) 274-0196 x72486\")\n Person(\"2217875886672\", \"Coletta\", \"Effertz\", \"Whitley.Bechtelar@mz.org\", \"Rippin Inc\", \"991.601.1323\")\n```\n\nNotice how we get a `Vector` of `Person`s... this is partially where that cool thing happens. \nPlacing that vector in a `DataFrame` constructor creates a dataframe object for us without any \nhassle at all:\n\n```julia\nusing DataFrames\n\ndf = DataFrame(list_of_five)\n```\n\n```\n5×6 DataFrame\n Row │ id             first_name  last_name     email                           company                           phone                 \n     │ String         String      String        String                          String                            String                \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ 502327436522   Simon       Lind          Franklyn.Satterfield@yahoo.com  Rutherford-Barton                 054.718.0236\n   2 │ 1988647737198  Charlott    Jacobs        Walter.Ziemann@hotmail.com      Towne, Gorczany and Brekke        839-605-0245 x477\n   3 │ 3335059941285  Glory       Brakus        Nienow.Cassandra@lh.net         Schuppe, Powlowski and Powlowski  (122) 872-3081 x3643\n   4 │ 4996530776723  Hedwig      Pfannerstill  wSchamberger@hg.net             Langosh Group                     (594) 274-0196 x72486\n   5 │ 2217875886672  Coletta     Effertz       Whitley.Bechtelar@mz.org        Rippin Inc                        991.601.1323\n```\n\nThat's pretty neat!\n\nAnyhow, with our basic functionality all set up, it's time to do some light benchmarking to \nget a sense of how this code will perform. I started off small by generating only 10,000 \nrecords:\n\n```julia\nusing BenchmarkTools\n\n@btime [get_person() for _ in 1:10_000] |> DataFrame;\n```\n\n```\n6.531 s (5158878 allocations: 253.88 MiB)\n```\n\nOof, that result is not very comforting -- taking 6.5 seconds just for 10,000 records does \nnot bode well. Assuming linear scaling (which we probably can't), it would take 6.5 * 100,000 \nseconds, or roughly 180 hours, to run the full thing.\n\nAt this point, I'm already thinking this whole idea is dead in the water, but was curious if a \nsolution existed and decided not to give up yet and keep exploring options.\n\n### Multi-threading\n\nThe first tool in the arsenal to reach for in such a situation is probabably is multi-threading. \n*Actually*, the first tool to reach for is proper profiling, but when it comes to parallelizing \ncode that is not ours, I would try multi-threading before exploring multi-processing or async.\n\nJulia has a really handy macro `Threads.@threads` that you can stick in front of a `for` loop \nto parallelize it. But using `Base` Julia to handcraft the threaded population of a vector is \n[a bit clunky](https://discourse.julialang.org/t/thread-safe-array-building/3275/2), \nin my opinion, thus I opted for the excellent [ThreadsX.jl](https://github.com/tkf/ThreadsX.jl) \npackage, which makes this particular task a breeze. In essence, it parallelizes certain `Base` \nfunctions (such as `sum`, `reduce`, `map`, `collect`, etc.) and all one needs to do is simply \nput `ThreadsX.` in front of the function. In our case, it looks like this (with the \nbenchmark result below):\n\n```julia\nusing ThreadsX\n\n@btime ThreadsX.collect(get_person() for _ in 1:100_000) |> DataFrame;\n```\n\n```\n1.072 s (5140629 allocations: 258.32 MiB)\n```\n\nOk so that's a little better, but running 12 threads and getting a 6x speed-up is not that \ngreat. More importantly, by our assumed linear scaling logic, the full 1 billion record run \nwould still take approximately 30 hours on my laptop, just to generate the data, nevermind \nserializing it to disk.\n\n\n### A quick benchmarking aside\n\nMy laptop has 8 physical / 16 logical cores. For some reason I set the default number of \nthreads to 12... I honestly don't know why. Perhaps I was thinking \"let's leave some for the \nothers\" `¯\\_(ツ)_/¯`.\n\nNow it's true that a lot of workloads won't take advantage of all logical threads that are \nsupposedly available, thus one is essentially bound to the number of physical cores, but some \nworkloads might take advantage of those extra threads. So to be thorough, I ran the following \nbenchmarks with 1 through 16 cores.\n\nI saved my code to a file named `fake_naive.jl`:\n\n```julia\nusing BenchmarkTools\n\n# --snip--\n\nprint(Threads.nthreads(), \"\\t\\t\")\n@btime ThreadsX.collect(get_person() for _ in 1:10_000) |> DataFrame;\n```\n\nand a short bash script as follows:\n\n```bash\n#!/bin/bash\n\nprintf 'Num Threads\\tBenchmark Results\\n'\n\nfor i in $(seq 1 16)\ndo\n    # start a julia process with i threads\n    julia -t \"$i\" -e 'include(\"fake_naive.jl\")'\ndone\n\n```\n\nRunning it gave the following results:\n```\nNum Threads     Benchmark Results\n1                 6.422 s (5146142 allocations: 256.55 MiB)\n2                 3.361 s (5146774 allocations: 256.92 MiB)\n3                 2.443 s (5141482 allocations: 257.37 MiB)\n4                 1.990 s (5149584 allocations: 257.76 MiB)\n5                 1.593 s (5138217 allocations: 257.54 MiB)\n6                 1.448 s (5140074 allocations: 257.62 MiB)\n7                 1.339 s (5141603 allocations: 257.70 MiB)\n8                 1.296 s (5141621 allocations: 257.69 MiB)\n9                 1.145 s (5139926 allocations: 258.29 MiB)\n10                1.112 s (5137139 allocations: 258.15 MiB)\n11                1.087 s (5146978 allocations: 258.64 MiB)\n12                1.050 s (5142290 allocations: 258.40 MiB)\n13                1.005 s (5137553 allocations: 258.16 MiB)\n14                966.497 ms (5139312 allocations: 258.26 MiB)\n15                955.672 ms (5147540 allocations: 258.66 MiB)\n16                906.340 ms (5140219 allocations: 258.30 MiB)\n```\n\nwith the plotted version:\n\n![](./benchmark_plot_01.svg)\n\nThis is interesting. Clearly we see diminishing returns as we increase the number of threads \npast 8 or so, nevertheless there is a 30% speed improvement going from 8 threads to 16 threads, \nat least according to this benchmark.\n\nOnce again, assuming linear scaling, we can expect 16 threads to run in 0.90634 / 6.422 * 180 \n= 25.5 hours. Not nearly good enough!\n\n\n### Finishing the naive code\n\nDespite knowing that my current approach is a losing battle, I wrote a function that generates \nthe data *and* saves it to parquet files, in order to test out IO as well:\n\n```julia\nusing Parquet2: writefile\n\nfunction save_the_people_sync(num_people, num_files)\n    for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/outfile_$(file_num).parquet\"\n        df = ThreadsX.collect(get_person() for _ in 1:num_people) |> DataFrame\n        writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n```\n\nalong with a second async version:\n\n```julia\nfunction save_the_people_async(num_people, num_files)\n    @sync for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/outfile_$(file_num).parquet\"\n        df = ThreadsX.collect(get_person() for _ in 1:num_people) |> DataFrame\n        @async writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n```\n\nThe idea behind the `@async` code above is that while dataset~n+1~ is being generated, \ndataset~n~ is being written to disk. At least in theory.\n\nBenchmarking the 10K records with 10 file writes (using 16 threads this time)\n\n```julia\n@btime save_the_people_sync(10_000, 10);\n@btime save_the_people_async(10_000, 10);\n```\n\ngave the following results:\n\n```\n  11.598 s (52145292 allocations: 2.57 GiB)\n  11.018 s (52179590 allocations: 2.57 GiB)\n```\n\nSo the async stuff doesn't really have much an effect, at least for 10K records. Trying it out \nwith 100K records results in:\n\n```\n  109.919 s (521194490 allocations: 25.58 GiB)\n  111.094 s (521135072 allocations: 25.58 GiB)\n```\n\nHmm... attempting to write bigger data asynchronously while using 16 threads to generate it \nactually resulted in a slight slowdown.\n\nI tried one more thing, because I was maybe over-subscribing my CPU -- I bumped my threads \ndown to 12 again and got 124.67s for the non-async and 123.13s for the async version. Clearly \nthe async wasn't helping much. Though we did get some confirmation that, for this use case, \nit makes sense to max out the number of available threads, seeing as how lowering the \nthread-count did result in a ~12 second slowdown.\n\nIn any case, it was time to move on.\n\n\n## Diving into Faker's Internals\n\nClearly doing a, more or less, 1-1 translation to Julia was not working. I posted my initial \nfindings on the Julia Slack, and one user's response inspired my next step:\n\n> ... I guess that the runtime is dominated by Faker, is it optimized for massive fake generation \nat all?\n\nI started looking into Faker.jl's internals and noticed a couple of things: it was not exactly \nidiomatic Julia and the API did not explicitly support passing in an argument for number of \nrecords generated (at least that I could observe). In other words, you have to call code like \nthis:\n\n```julia\nlast_name = Faker.last_name()\nemail     = Faker.email()\ncompany   = Faker.company()\n```\n\nand not like this:\n\n```julia\nN = 1_000_000\n\nlast_names = Faker.last_name(N)\nemails     = Faker.email(N)\ncompanies  = Faker.company(N)\n```\n\nMeaning with each function call, you can only produce one record at a time. If you wanted to \ngenerate 1 million records, you would have to do 1 million function calls. And the way you'd \ndo that is with a loop or comprehension, as I did above. But these million function calls \nmight be more expensive than necessary.\n\nThe first thing I noticed about Faker.jl is that the data that it samples from, such as actual \nfirst and last names, is stored as text in YAML files, which makes sense. And for items such \nas email addresses and phone numbers, it stores a kind of pattern which then gets translated \nby Julia functions into (randomly generated) text.\n\nFor instance, the `name.yml` file contains this:\n\n```yaml\nen:\n  faker:\n    name:\n      male_first_name: [Aaron, Abdul, Abe, Abel, Abraham...]\n      female_first_name: [Abbey, Abbie, Abby, Abigail, Ada...]\n      first_name:\n        - \"#{female_first_name}\"\n        - \"#{male_first_name}\"\n      last_name: [Abbott, Abernathy, Abshire, Adams...]\n      ...\n```\n\nThe function definitions for first names are:\n\n```julia\nmale_first_name()::String = executor(data[\"faker\"][\"name\"][\"male_first_name\"])\nfemale_first_name()::String = executor(data[\"faker\"][\"name\"][\"female_first_name\"])\n\nfunction first_name(genere::String=\"None\")::String\n    if(cmp(genere, \"M\") == 0)\n        return male_first_name()\n    elseif(cmp(genere, \"F\") == 0)\n        return female_first_name()\n    end\n end\n```\n\nThe `data` variable above is a dictionary holding all the data and patterns. When the package \nis first loaded, `data` is initialized as a global variable that is an empty dictionary of \ntype `Dict{Any, Any}` and is then populated with the contents of the YAML files. It being a \nnon-constant global variable may have something to do with the performance issues -- one of the \nmain rules of [writing performant Julia](https://docs.julialang.org/en/v1/manual/performance-tips/#Avoid-untyped-global-variables) \nis **don't use non-constant untyped global variables**.\n\nIn any case, calling\n\n```julia\nFaker.data[\"faker\"][\"name\"][\"female_first_name\"]\n```\n\nsimply returns a vector of names:\n```\n4272-element Vector{String}:\n \"Abbey\"\n \"Abbie\"\n \"Abby\"\n \"Abigail\"\n \"Ada\"\n ⋮\n \"Zoraida\"\n \"Zula\"\n \"Zulema\"\n \"Zulma\"\n```\n\nand all the `executor` function does is pick a name at random (though that's not all it does, \nnot for other types of data):\n\n```julia\nFaker.executor(Faker.data[\"faker\"][\"name\"][\"female_first_name\"])\n```\n```\n\"Estela\"\n```\n\nChecking this function for type stability with `@code_warntype` shows that it's not type \nstable, though again I'm not sure if that's a problem here (it may be, but I'd have to do some \nmore testing, which I haven't had the time to do).\n\nAnyhow, this gave me an idea and I decided to run some more benchmarks.\n\n## The Revelation\n\nTo isolate the impact of just `Faker.first_name()` being called a million times, I ran the \nfollowing:\n\n```julia\n@btime [Faker.first_name() for _ in 1:1_000_000];\n```\n```\n  75.036 s (58000002 allocations: 2.84 GiB)\n```\n\nMy idea was to see if sampling from a vector of names using Julia's `rand` function was \nperhaps faster. To do that, I had to concat the male and female first name vectors and then \nsample that new vector:\n\n```julia\nfunction get_first_names(n::Int)\n    first_names_female = Faker.data[\"faker\"][\"name\"][\"female_first_name\"]\n    first_names_male = Faker.data[\"faker\"][\"name\"][\"male_first_name\"]\n    first_names_all = vcat(first_names_female, first_names_male)\n    return rand(first_names_all, n)\nend\n```\n\nBenchmarking this function yielded quite the surprise:\n\n```julia\n@btime first_name = get_first_names(1_000_000);\n```\n```\n  4.195 ms (5 allocations: 7.67 MiB)\n```\n\nWow.\n\nFour milliseconds vs 75 seconds. That is an almost 18,000x speed-up! And notice the \nallocations and memory usage as well: 5 allocations vs 58 million, and ~8MiB vs 2.84GiB.\n\nThis was the magic sauce I had been looking for all along.\n\n\n## Julia: Second Attempt\n\nI won't go into many more details of the Faker.jl internals because a) I'm still getting to \nknow the package, and b) I'm not sure they're all that relevant or interesting for the purpose \nof this write-up (perhaps another blogpost).\n\nI'd like to take a moment though to say that **I sincerely hope this post does not come off \nas harsh or unduly critical of Faker.jl and its author**. For one, I admire anyone who spends \ntheir free time writing open-source software, and in the case of Faker.jl, it fills a gap in \nthe Julia ecosystem that, to my knowledge, no one else has stepped up to fill. Moreover, it \nhas very nice documentation, albeit simple (and appropriately so). And last but far from least, \n[the author appears to be kind and friendly ](https://github.com/neomatrixcode/Faker.jl/issues/30#issuecomment-1045495757), \nwhich counts for a lot in my book.\n\nThe next step was to write my own functions for each of the `Person` fields and benchmark \nthem against the package-provided equivalents.\n\n\n#### id\n\nThis is perhaps the simplest one, it's just using `rand` to sample from a range. In the \noriginal Python version, there was no requirement to have the `id` field as a string (though \nif I was designing that database and using random integers, I would be sure to turn them into \nstrings and left-pad with zeros).\n\n```julia\nget_id(n::Int) = rand(1000:9999999999999, n)\n```\n\n\n#### last_name\n\nThis one is almost the same as the `first_name` function, only we don't have to concatenate \ntwo vectors together.\n\n```julia\nfunction get_last_names(n::Int)\n    last_names = Faker.data[\"faker\"][\"name\"][\"last_name\"]\n    return rand(last_names, n)\nend\n```\n\n\n#### email\n\nThis one required a little bit more work. Instead of relying on the internal package mechanism \nto generate fake email addresses, I rolled my own.\n\nIn the first function, closures provide each of the possible email formats. A closure is \nrandomly selected, and the function arguments, `first_name` and `last_name`, are used to \ngenerate a username.\n\n```julia\nfunction determine_username_format(first_name::String, last_name::String)\n    # create closure for each type of format\n    _first_last(first_name, last_name) = first_name * \".\" * last_name\n    _last_first(first_name, last_name) = last_name * \".\" * first_name\n    _first_num(first_name, last_name) = first_name * string(rand(1:99))\n    _flast(first_name, last_name) = first(first_name, 1) * last_name\n    # randomly sample each format and return, not just function, but result from function\n    formats = (_first_last, _last_first, _first_num, _flast)\n    _email_format(first_name, last_name) = rand(formats)(first_name, last_name)\n    return _email_format(first_name, last_name) |> lowercase\nend\n```\n\nNote that this function only works on scalar inputs and will be broadcast to vectors of first \nand last names later. Also, note how `_first_num` takes two arguments but only uses one -- we \nneed that to satisfy our mini `_email_format` API. If `_first_num` was defined with only the \n`first_name` argument, and that function was randomly selected for any given iteration, it \nwould bomb. Thus we leave it with the same args as all the other inner functions. I know it \nfeels hacky (and there is probably a name for this technique) but it simplifies the code.\n\nMoving on, although there are more than three email domains (in the real world and in the \nFaker packages), I kept things simple and only used the main three that each account for ~17% \nof market share (according to my quick googling on the topic). I know this is cheating a \nlittle bit, but I honestly ran out of patience and figured this was good enough to get an \nidea of how fast this code can go.\n\n```julia\ndetermine_email_domains(n::Int) = rand((\"@gmail.com\", \"@yahoo.com\", \"@hotmail.com\"), n)\n```\n\nFinally, we take the vectors of names that are already generated and use them to call the two \nhelper functions above (using the `.` notation to broadcast the `determine_username_format` \nfunction).\n\n```julia\nfunction get_emails(first_names::Vector{String}, last_names::Vector{String})\n    n = size(first_names, 1)\n    @assert n == size(last_names, 1) \"first and last name vectors must be same size\"\n\n    formats = determine_username_format.(first_names, last_names)\n    domains = determine_email_domains(n)\n\n    return formats .* domains\nend\n```\n\nYou'll notice that this is a deviation from the original post in that people's names and their \nemail addresses are now consistent since their names were used to generate their emails. This \nis an enhancement that is partly born out of convenience (we're already generating first and last \nnames, why spend more time generating more?) and partly because if we're getting our hands \ndirty with hand-tuned optimization, we might as well make the dataset slightly more realistic. \nSo we lose some (only three domains) and win some (email addresses based off the names).\n\nI haven't tested whether the closures are more helpful here, instead of regular functions, \nbut I'm sort of keeping them as is mainly to keep the code somewhat organized. They also don't \nneed to start with an `_` but, again, helps me visually parse them a little easier.\n\n\n#### company\n\nSimilar to the email functions above, I use closures to hold the pattern for the various \ncompany name formats, and then randomly select one of them to generate the pattern. Also, the \n`get_last_names` function gets re-used (yay).\n\n```julia\nfunction get_companies(n::Int)\n    suffixes = (\" Inc\", \" and Sons\", \" and Daughters\", \", LLC\", \" Group\")\n\n    _last_suffix(ln1, ln2, ln3) = ln1 * rand(suffixes)\n    _last_last(ln1, ln2, ln3) = ln1 * \"-\" * ln2\n    _last_x3(ln1, ln2, ln3) = ln1 * \", \" * ln2 * \" and \" * ln3\n    formats = (_last_suffix, _last_last, _last_x3)\n\n    last_names1 = get_last_names(n)\n    last_names2 = get_last_names(n)\n    last_names3 = get_last_names(n)\n\n    _company_name(ln1, ln2, ln3) = rand(formats)(ln1, ln2, ln3)\n\n    return _company_name.(last_names1, last_names2, last_names3)\nend\n```\n\nOnce again, the inner functions don't all use the given arguments, but we need that for the \nmini `_company_name` API.\n\n\n#### phone\n\nThis one was kept pretty much as is from the package, and that's fine because as we'll see its \nperformance is just fine as it is.\n\n```julia\nfunction get_phone_numbers(n::Int)\n    return [Faker.phone_number() for _ in 1:n]\nend\n```\n\n#### Save All the People\n\nPutting it all together, we get this:\n\n```julia\nfunction get_the_people(n::Int)\n    df = DataFrame(\n        id = get_id(n),\n        first_name = get_first_names(n),\n        last_name = get_last_names(n),\n        company= get_companies(n),\n        phone_number = get_phone_numbers(n),\n    )\n    df.email = get_emails(df.first_name, df.last_name)\n    return df\nend\n```\n\nAs you can see, we dispensed with the `Person` struct since we don't really need it. Instead, \nwe generate a `DataFrame` directly. And because we need our first and last names before we can \ngenerate our email addresses, the email field comes last.\n\nFinally, we have actual data generation and saving to parquet files.\n\n```julia\nfunction save_all_the_people(num_people::Int, num_files::Int)\n    for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/persons_$(file_num).parquet\"\n        df = get_the_people(num_people)\n        writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n```\n\nNow it's time to benchmark again.\n\n## The Final Benchmarks\n\nBefore we benchmark the big one above, we'll do some micro-benchmarking and test the \nfunctions that create each of the fields:\n\n```julia\n## benchmarks\nconst N = 1_000_000\n\n# get_emails depends on other fields being generated first, \n# which needs to be done outside of benchmarking\nfirst_names = get_first_names(N)\nlast_names = get_last_names(N)\n\n@btime get_id($N);\n@btime get_first_names($N);\n@btime get_last_names($N);\n@btime get_emails($first_names, $last_names);\n@btime get_companies($N);\n@btime get_phone_numbers($N);\n@btime get_the_people($N);\n```\n```\n  3.186 ms (2 allocations: 7.63 MiB)\n  4.270 ms (5 allocations: 7.67 MiB)\n  4.134 ms (3 allocations: 7.63 MiB)\n  309.268 ms (5747839 allocations: 262.10 MiB)\n  80.762 ms (2331987 allocations: 128.40 MiB)\n  367.853 ms (3000002 allocations: 187.66 MiB)\n  812.161 ms (11082578 allocations: 645.02 MiB)\n```\n\nLooks like we'll be spending the most time generating emails and phone numbers. We'll see if \nwe can cut that down eventually.\n\nTesting the big one:\n\n```julia\n@btime save_all_the_people(N, 10);\n```\n```\n  16.852 s (170856196 allocations: 10.43 GiB)\n```\n\nOk, not bad, generating 10 million people and saving to disk takes 17 seconds. But let's see \nif we can do better. We'll try multi-threading again, but this time just the `@threads` macro \nin front of the `for` loop should be good enough:\n\n```julia\nfunction save_all_the_people_threaded(num_people::Int, num_files::Int)\n    Threads.@threads for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/persons_$(file_num).parquet\"\n        df = get_the_people(num_people)\n        writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n```\n\nWe'll go for 16 threads right out of the gate:\n\n```julia\n@btime save_all_the_people_threaded(N, 10);\n```\n```\n  4.709 s (170854140 allocations: 10.43 GiB)\n```\n\nNot bad at all, though we're only getting a 3x speed-up. Let's try with 100 million:\n\n```julia\n@btime save_all_the_people_threaded(N, 100);\n```\n\nHere's a screenshot of `htop` while this is happening btw:\n![](htop.png)\n\nBenchmark result of 100 million people:\n\n```\n  94.074 s (1708559591 allocations: 104.34 GiB)\n```\n\nOk interesting, looks like we can't assume linear scaling after all! By that logic, 100 \nmillion should have taken about 50 seconds, but it actually took about twice that.\n\nNot to belabor the point, **doing an actual run with 1 billion people (1,000 parquet files) \ntook about 23 minutes!**\n\n\n## Concluding Thoughts\n\nTurns out we can indeed speed up the generation of fake data by quite a bit using plain Julia \nand a moderately powered laptop.\n\nI might explore further optimization opportunities and do some proper profiling in future \nposts. For completeness, it would behoove me to play around with multi-processing, which I \ndidn't here since it's more of a hassle and I doubt it would yield any appreciable performance \nimprovement over the threaded version. I suspect there are significant gains to be made just \nhand-tuning the serial code, especially the string handling in the `get_emails` and \n`get_phone_numbers` functions.\n\nI can imagine someone arguing, \"but you probably spent more time researching this issue and \nwriting custom code... surely $12 and 2 hours on an EC2 makes more sense for a one-time run \nlike this.\"\n\nAbsolutely. I definitely spent more than 2 hours looking into Faker.jl and coming up with \nmy own. And if we were just trying to get a job done, that would be the way to go. But... \nwhere's the fun in that?\n\nThis way, we learned a thing or two (or at least I did). And most importantly we are now armed \nwith enough knowledge to make a PR or two to the Faker.jl repo and have the whole Julia and \ndata communities benefit. If time allows, that is indeed what I'll be looking to do.\n\nLastly, I also hope that any newbie Julia programmers or various data practioners (data \nenginners, data scientists, data analysits) will benefit from reading this post and learn a \nthing or two themselves.\n\nCheers and happy coding :)\n\n\n## Appendix: Software and Hardware\n\nThe code in this post was run with Julia 1.8.5 and the following package versions:\n\n```julia\nusing Pkg\n\nPkg.status()\n```\n```\nStatus `~/Dev/Julia/FakePeople/Project.toml`\n  [a93c6f00] DataFrames v1.5.0\n  [0efc519c] Faker v0.3.5\n  [98572fba] Parquet2 v0.2.9\n  [ac1d9e8a] ThreadsX v0.1.11\n```\n\nwith the following hardware/software specs:\n\n```julia\nversioninfo()\n```\n```\nJulia Version 1.8.5\nCommit 17cfb8e65ea (2023-01-08 06:45 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 × AMD Ryzen 7 PRO 4750U with Radeon Graphics\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-13.0.1 (ORCJIT, znver2)\n  Threads: 12 on 16 virtual cores\nEnvironment:\n  JULIA_NUM_THREADS = 12\n  JULIA_EDITOR = code\n```\n","srcMarkdownNoYaml":"\n\nIn a [recent MotherDuck blog post](https://motherduck.com/blog/python-faker-duckdb-exploration/), \nthe author generated 1 billion fake people records using Python in order to analyze the data \nwith DuckDB. I suspect the point of the article was to showcase how awesome `duckdb` is at \nhandling large amounts of local data, but it did spend the majority of its time explaining the \ndata generation process, which made for a fun read.\n\nOne of the more interesting tidbits from the article was:\n\n> I used the GNU Parallel technique discussed above with a hefty m6i.32xlarge instance on \nAmazon EC2, though generated a billion people in 1k parquet files. This took about 2 hours to \ngenerate.\n\nYikes, that's a lot of firepower! [That machine]((https://instances.vantage.sh/aws/ec2/m6i.32xlarge)) \ncomes with 128 vCPUs and 512 GiB RAM, and costs about $6 an hour. So pretty hefty indeed.\n\nBeing a big fan of Julia, I decided to see what it would be like to generate such a dataset \nwith Julia. More concretely, I wanted to see if I can use less resources (only my laptop) *and* \nhave the process run in significantly less time. Not only that, I thought it be fun to explore \nthe various modes of concurrency (multi-threading, multi-processing, async, etc) Julia offers.\n\nThe results were initially disappointing, as I'll explain below, but in the end I did get that \nnice speed-up I was looking for.\n\n\n## The Details\n\nThe [original post](https://motherduck.com/blog/python-faker-duckdb-exploration/) has the full \ndetails, but I'll go over the basic details here. A `person` record consists of the following \nrandomly generated fields:\n```\n- id\n- first_name\n- last_name\n- email\n- company\n- phone\n```\n\nUsing the Python [Faker](https://faker.readthedocs.io/en/master/) library to generate the \ndata and [GNU Parallel](https://www.gnu.org/software/parallel/) to parallelize the operation, \nthe author created 1,000 parquet files with 1 million records each before populating a \n`duckdb` database for further analysis.\n\n\n## Julia: First Attempt\n\nLuckily, Julia has its own [Faker.jl](https://github.com/neomatrixcode/Faker.jl) package. \nUsing it is as simple as:\n\n```julia\nusing Faker\n\nFaker.first_name()\n```\n\n```\n\"Wilfredo\"\n```\n\nInstead of putting all the fields in a dictionary, I created a struct instead:\n\n```julia\nstruct Person\n    id::String\n    first_name::String\n    last_name::String\n    email::String\n    company::String\n    phone::String\nend\n```\n\nAside from being a natural thing to do in Julia, this ended up being a really handy vehicle \nfor populating a DataFrame, as we'll see in a moment.\n\nIn order to construct the `Person` object, we have the following function, which is essentially \nthe same as in the Python version in the original post:\n\n```julia\nfunction get_person()\n    person = Person(\n        Faker.random_int(min=1000, max=9999999999999),\n        Faker.first_name(),\n        Faker.last_name(),\n        Faker.email(),\n        Faker.company(),\n        Faker.phone_number()\n    )\n    return person\nend\n\nget_person()\n```\n\n```\nPerson(\"8429894898777\", \"Christin\", \"Gleason\", \"Archie99@yahoo.com\", \"Mante, Hilll and Hessel\", \"1-548-869-5799 x26945\")\n```\n\n\nThis approach clearly suffers from the same deficiency as the original in that the generated \nemail address bears absolutely no semblance to the generated first and last names. But \nthat's ok, we're just making up data for mocking and testing purposes anyhow.\n\nTo create an array of `Person`s, we can use a comprehension:\n\n```julia\nlist_of_five = [get_person() for _ in 1:5]\n```\n\n```\n5-element Vector{Person}:\n Person(\"502327436522\", \"Simon\", \"Lind\", \"Franklyn.Satterfield@yahoo.com\", \"Rutherford-Barton\", \"054.718.0236\")\n Person(\"1988647737198\", \"Charlott\", \"Jacobs\", \"Walter.Ziemann@hotmail.com\", \"Towne, Gorczany and Brekke\", \"839-605-0245 x477\")\n Person(\"3335059941285\", \"Glory\", \"Brakus\", \"Nienow.Cassandra@lh.net\", \"Schuppe, Powlowski and Powlowski\", \"(122) 872-3081 x3643\")\n Person(\"4996530776723\", \"Hedwig\", \"Pfannerstill\", \"wSchamberger@hg.net\", \"Langosh Group\", \"(594) 274-0196 x72486\")\n Person(\"2217875886672\", \"Coletta\", \"Effertz\", \"Whitley.Bechtelar@mz.org\", \"Rippin Inc\", \"991.601.1323\")\n```\n\nNotice how we get a `Vector` of `Person`s... this is partially where that cool thing happens. \nPlacing that vector in a `DataFrame` constructor creates a dataframe object for us without any \nhassle at all:\n\n```julia\nusing DataFrames\n\ndf = DataFrame(list_of_five)\n```\n\n```\n5×6 DataFrame\n Row │ id             first_name  last_name     email                           company                           phone                 \n     │ String         String      String        String                          String                            String                \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ 502327436522   Simon       Lind          Franklyn.Satterfield@yahoo.com  Rutherford-Barton                 054.718.0236\n   2 │ 1988647737198  Charlott    Jacobs        Walter.Ziemann@hotmail.com      Towne, Gorczany and Brekke        839-605-0245 x477\n   3 │ 3335059941285  Glory       Brakus        Nienow.Cassandra@lh.net         Schuppe, Powlowski and Powlowski  (122) 872-3081 x3643\n   4 │ 4996530776723  Hedwig      Pfannerstill  wSchamberger@hg.net             Langosh Group                     (594) 274-0196 x72486\n   5 │ 2217875886672  Coletta     Effertz       Whitley.Bechtelar@mz.org        Rippin Inc                        991.601.1323\n```\n\nThat's pretty neat!\n\nAnyhow, with our basic functionality all set up, it's time to do some light benchmarking to \nget a sense of how this code will perform. I started off small by generating only 10,000 \nrecords:\n\n```julia\nusing BenchmarkTools\n\n@btime [get_person() for _ in 1:10_000] |> DataFrame;\n```\n\n```\n6.531 s (5158878 allocations: 253.88 MiB)\n```\n\nOof, that result is not very comforting -- taking 6.5 seconds just for 10,000 records does \nnot bode well. Assuming linear scaling (which we probably can't), it would take 6.5 * 100,000 \nseconds, or roughly 180 hours, to run the full thing.\n\nAt this point, I'm already thinking this whole idea is dead in the water, but was curious if a \nsolution existed and decided not to give up yet and keep exploring options.\n\n### Multi-threading\n\nThe first tool in the arsenal to reach for in such a situation is probabably is multi-threading. \n*Actually*, the first tool to reach for is proper profiling, but when it comes to parallelizing \ncode that is not ours, I would try multi-threading before exploring multi-processing or async.\n\nJulia has a really handy macro `Threads.@threads` that you can stick in front of a `for` loop \nto parallelize it. But using `Base` Julia to handcraft the threaded population of a vector is \n[a bit clunky](https://discourse.julialang.org/t/thread-safe-array-building/3275/2), \nin my opinion, thus I opted for the excellent [ThreadsX.jl](https://github.com/tkf/ThreadsX.jl) \npackage, which makes this particular task a breeze. In essence, it parallelizes certain `Base` \nfunctions (such as `sum`, `reduce`, `map`, `collect`, etc.) and all one needs to do is simply \nput `ThreadsX.` in front of the function. In our case, it looks like this (with the \nbenchmark result below):\n\n```julia\nusing ThreadsX\n\n@btime ThreadsX.collect(get_person() for _ in 1:100_000) |> DataFrame;\n```\n\n```\n1.072 s (5140629 allocations: 258.32 MiB)\n```\n\nOk so that's a little better, but running 12 threads and getting a 6x speed-up is not that \ngreat. More importantly, by our assumed linear scaling logic, the full 1 billion record run \nwould still take approximately 30 hours on my laptop, just to generate the data, nevermind \nserializing it to disk.\n\n\n### A quick benchmarking aside\n\nMy laptop has 8 physical / 16 logical cores. For some reason I set the default number of \nthreads to 12... I honestly don't know why. Perhaps I was thinking \"let's leave some for the \nothers\" `¯\\_(ツ)_/¯`.\n\nNow it's true that a lot of workloads won't take advantage of all logical threads that are \nsupposedly available, thus one is essentially bound to the number of physical cores, but some \nworkloads might take advantage of those extra threads. So to be thorough, I ran the following \nbenchmarks with 1 through 16 cores.\n\nI saved my code to a file named `fake_naive.jl`:\n\n```julia\nusing BenchmarkTools\n\n# --snip--\n\nprint(Threads.nthreads(), \"\\t\\t\")\n@btime ThreadsX.collect(get_person() for _ in 1:10_000) |> DataFrame;\n```\n\nand a short bash script as follows:\n\n```bash\n#!/bin/bash\n\nprintf 'Num Threads\\tBenchmark Results\\n'\n\nfor i in $(seq 1 16)\ndo\n    # start a julia process with i threads\n    julia -t \"$i\" -e 'include(\"fake_naive.jl\")'\ndone\n\n```\n\nRunning it gave the following results:\n```\nNum Threads     Benchmark Results\n1                 6.422 s (5146142 allocations: 256.55 MiB)\n2                 3.361 s (5146774 allocations: 256.92 MiB)\n3                 2.443 s (5141482 allocations: 257.37 MiB)\n4                 1.990 s (5149584 allocations: 257.76 MiB)\n5                 1.593 s (5138217 allocations: 257.54 MiB)\n6                 1.448 s (5140074 allocations: 257.62 MiB)\n7                 1.339 s (5141603 allocations: 257.70 MiB)\n8                 1.296 s (5141621 allocations: 257.69 MiB)\n9                 1.145 s (5139926 allocations: 258.29 MiB)\n10                1.112 s (5137139 allocations: 258.15 MiB)\n11                1.087 s (5146978 allocations: 258.64 MiB)\n12                1.050 s (5142290 allocations: 258.40 MiB)\n13                1.005 s (5137553 allocations: 258.16 MiB)\n14                966.497 ms (5139312 allocations: 258.26 MiB)\n15                955.672 ms (5147540 allocations: 258.66 MiB)\n16                906.340 ms (5140219 allocations: 258.30 MiB)\n```\n\nwith the plotted version:\n\n![](./benchmark_plot_01.svg)\n\nThis is interesting. Clearly we see diminishing returns as we increase the number of threads \npast 8 or so, nevertheless there is a 30% speed improvement going from 8 threads to 16 threads, \nat least according to this benchmark.\n\nOnce again, assuming linear scaling, we can expect 16 threads to run in 0.90634 / 6.422 * 180 \n= 25.5 hours. Not nearly good enough!\n\n\n### Finishing the naive code\n\nDespite knowing that my current approach is a losing battle, I wrote a function that generates \nthe data *and* saves it to parquet files, in order to test out IO as well:\n\n```julia\nusing Parquet2: writefile\n\nfunction save_the_people_sync(num_people, num_files)\n    for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/outfile_$(file_num).parquet\"\n        df = ThreadsX.collect(get_person() for _ in 1:num_people) |> DataFrame\n        writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n```\n\nalong with a second async version:\n\n```julia\nfunction save_the_people_async(num_people, num_files)\n    @sync for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/outfile_$(file_num).parquet\"\n        df = ThreadsX.collect(get_person() for _ in 1:num_people) |> DataFrame\n        @async writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n```\n\nThe idea behind the `@async` code above is that while dataset~n+1~ is being generated, \ndataset~n~ is being written to disk. At least in theory.\n\nBenchmarking the 10K records with 10 file writes (using 16 threads this time)\n\n```julia\n@btime save_the_people_sync(10_000, 10);\n@btime save_the_people_async(10_000, 10);\n```\n\ngave the following results:\n\n```\n  11.598 s (52145292 allocations: 2.57 GiB)\n  11.018 s (52179590 allocations: 2.57 GiB)\n```\n\nSo the async stuff doesn't really have much an effect, at least for 10K records. Trying it out \nwith 100K records results in:\n\n```\n  109.919 s (521194490 allocations: 25.58 GiB)\n  111.094 s (521135072 allocations: 25.58 GiB)\n```\n\nHmm... attempting to write bigger data asynchronously while using 16 threads to generate it \nactually resulted in a slight slowdown.\n\nI tried one more thing, because I was maybe over-subscribing my CPU -- I bumped my threads \ndown to 12 again and got 124.67s for the non-async and 123.13s for the async version. Clearly \nthe async wasn't helping much. Though we did get some confirmation that, for this use case, \nit makes sense to max out the number of available threads, seeing as how lowering the \nthread-count did result in a ~12 second slowdown.\n\nIn any case, it was time to move on.\n\n\n## Diving into Faker's Internals\n\nClearly doing a, more or less, 1-1 translation to Julia was not working. I posted my initial \nfindings on the Julia Slack, and one user's response inspired my next step:\n\n> ... I guess that the runtime is dominated by Faker, is it optimized for massive fake generation \nat all?\n\nI started looking into Faker.jl's internals and noticed a couple of things: it was not exactly \nidiomatic Julia and the API did not explicitly support passing in an argument for number of \nrecords generated (at least that I could observe). In other words, you have to call code like \nthis:\n\n```julia\nlast_name = Faker.last_name()\nemail     = Faker.email()\ncompany   = Faker.company()\n```\n\nand not like this:\n\n```julia\nN = 1_000_000\n\nlast_names = Faker.last_name(N)\nemails     = Faker.email(N)\ncompanies  = Faker.company(N)\n```\n\nMeaning with each function call, you can only produce one record at a time. If you wanted to \ngenerate 1 million records, you would have to do 1 million function calls. And the way you'd \ndo that is with a loop or comprehension, as I did above. But these million function calls \nmight be more expensive than necessary.\n\nThe first thing I noticed about Faker.jl is that the data that it samples from, such as actual \nfirst and last names, is stored as text in YAML files, which makes sense. And for items such \nas email addresses and phone numbers, it stores a kind of pattern which then gets translated \nby Julia functions into (randomly generated) text.\n\nFor instance, the `name.yml` file contains this:\n\n```yaml\nen:\n  faker:\n    name:\n      male_first_name: [Aaron, Abdul, Abe, Abel, Abraham...]\n      female_first_name: [Abbey, Abbie, Abby, Abigail, Ada...]\n      first_name:\n        - \"#{female_first_name}\"\n        - \"#{male_first_name}\"\n      last_name: [Abbott, Abernathy, Abshire, Adams...]\n      ...\n```\n\nThe function definitions for first names are:\n\n```julia\nmale_first_name()::String = executor(data[\"faker\"][\"name\"][\"male_first_name\"])\nfemale_first_name()::String = executor(data[\"faker\"][\"name\"][\"female_first_name\"])\n\nfunction first_name(genere::String=\"None\")::String\n    if(cmp(genere, \"M\") == 0)\n        return male_first_name()\n    elseif(cmp(genere, \"F\") == 0)\n        return female_first_name()\n    end\n end\n```\n\nThe `data` variable above is a dictionary holding all the data and patterns. When the package \nis first loaded, `data` is initialized as a global variable that is an empty dictionary of \ntype `Dict{Any, Any}` and is then populated with the contents of the YAML files. It being a \nnon-constant global variable may have something to do with the performance issues -- one of the \nmain rules of [writing performant Julia](https://docs.julialang.org/en/v1/manual/performance-tips/#Avoid-untyped-global-variables) \nis **don't use non-constant untyped global variables**.\n\nIn any case, calling\n\n```julia\nFaker.data[\"faker\"][\"name\"][\"female_first_name\"]\n```\n\nsimply returns a vector of names:\n```\n4272-element Vector{String}:\n \"Abbey\"\n \"Abbie\"\n \"Abby\"\n \"Abigail\"\n \"Ada\"\n ⋮\n \"Zoraida\"\n \"Zula\"\n \"Zulema\"\n \"Zulma\"\n```\n\nand all the `executor` function does is pick a name at random (though that's not all it does, \nnot for other types of data):\n\n```julia\nFaker.executor(Faker.data[\"faker\"][\"name\"][\"female_first_name\"])\n```\n```\n\"Estela\"\n```\n\nChecking this function for type stability with `@code_warntype` shows that it's not type \nstable, though again I'm not sure if that's a problem here (it may be, but I'd have to do some \nmore testing, which I haven't had the time to do).\n\nAnyhow, this gave me an idea and I decided to run some more benchmarks.\n\n## The Revelation\n\nTo isolate the impact of just `Faker.first_name()` being called a million times, I ran the \nfollowing:\n\n```julia\n@btime [Faker.first_name() for _ in 1:1_000_000];\n```\n```\n  75.036 s (58000002 allocations: 2.84 GiB)\n```\n\nMy idea was to see if sampling from a vector of names using Julia's `rand` function was \nperhaps faster. To do that, I had to concat the male and female first name vectors and then \nsample that new vector:\n\n```julia\nfunction get_first_names(n::Int)\n    first_names_female = Faker.data[\"faker\"][\"name\"][\"female_first_name\"]\n    first_names_male = Faker.data[\"faker\"][\"name\"][\"male_first_name\"]\n    first_names_all = vcat(first_names_female, first_names_male)\n    return rand(first_names_all, n)\nend\n```\n\nBenchmarking this function yielded quite the surprise:\n\n```julia\n@btime first_name = get_first_names(1_000_000);\n```\n```\n  4.195 ms (5 allocations: 7.67 MiB)\n```\n\nWow.\n\nFour milliseconds vs 75 seconds. That is an almost 18,000x speed-up! And notice the \nallocations and memory usage as well: 5 allocations vs 58 million, and ~8MiB vs 2.84GiB.\n\nThis was the magic sauce I had been looking for all along.\n\n\n## Julia: Second Attempt\n\nI won't go into many more details of the Faker.jl internals because a) I'm still getting to \nknow the package, and b) I'm not sure they're all that relevant or interesting for the purpose \nof this write-up (perhaps another blogpost).\n\nI'd like to take a moment though to say that **I sincerely hope this post does not come off \nas harsh or unduly critical of Faker.jl and its author**. For one, I admire anyone who spends \ntheir free time writing open-source software, and in the case of Faker.jl, it fills a gap in \nthe Julia ecosystem that, to my knowledge, no one else has stepped up to fill. Moreover, it \nhas very nice documentation, albeit simple (and appropriately so). And last but far from least, \n[the author appears to be kind and friendly ](https://github.com/neomatrixcode/Faker.jl/issues/30#issuecomment-1045495757), \nwhich counts for a lot in my book.\n\nThe next step was to write my own functions for each of the `Person` fields and benchmark \nthem against the package-provided equivalents.\n\n\n#### id\n\nThis is perhaps the simplest one, it's just using `rand` to sample from a range. In the \noriginal Python version, there was no requirement to have the `id` field as a string (though \nif I was designing that database and using random integers, I would be sure to turn them into \nstrings and left-pad with zeros).\n\n```julia\nget_id(n::Int) = rand(1000:9999999999999, n)\n```\n\n\n#### last_name\n\nThis one is almost the same as the `first_name` function, only we don't have to concatenate \ntwo vectors together.\n\n```julia\nfunction get_last_names(n::Int)\n    last_names = Faker.data[\"faker\"][\"name\"][\"last_name\"]\n    return rand(last_names, n)\nend\n```\n\n\n#### email\n\nThis one required a little bit more work. Instead of relying on the internal package mechanism \nto generate fake email addresses, I rolled my own.\n\nIn the first function, closures provide each of the possible email formats. A closure is \nrandomly selected, and the function arguments, `first_name` and `last_name`, are used to \ngenerate a username.\n\n```julia\nfunction determine_username_format(first_name::String, last_name::String)\n    # create closure for each type of format\n    _first_last(first_name, last_name) = first_name * \".\" * last_name\n    _last_first(first_name, last_name) = last_name * \".\" * first_name\n    _first_num(first_name, last_name) = first_name * string(rand(1:99))\n    _flast(first_name, last_name) = first(first_name, 1) * last_name\n    # randomly sample each format and return, not just function, but result from function\n    formats = (_first_last, _last_first, _first_num, _flast)\n    _email_format(first_name, last_name) = rand(formats)(first_name, last_name)\n    return _email_format(first_name, last_name) |> lowercase\nend\n```\n\nNote that this function only works on scalar inputs and will be broadcast to vectors of first \nand last names later. Also, note how `_first_num` takes two arguments but only uses one -- we \nneed that to satisfy our mini `_email_format` API. If `_first_num` was defined with only the \n`first_name` argument, and that function was randomly selected for any given iteration, it \nwould bomb. Thus we leave it with the same args as all the other inner functions. I know it \nfeels hacky (and there is probably a name for this technique) but it simplifies the code.\n\nMoving on, although there are more than three email domains (in the real world and in the \nFaker packages), I kept things simple and only used the main three that each account for ~17% \nof market share (according to my quick googling on the topic). I know this is cheating a \nlittle bit, but I honestly ran out of patience and figured this was good enough to get an \nidea of how fast this code can go.\n\n```julia\ndetermine_email_domains(n::Int) = rand((\"@gmail.com\", \"@yahoo.com\", \"@hotmail.com\"), n)\n```\n\nFinally, we take the vectors of names that are already generated and use them to call the two \nhelper functions above (using the `.` notation to broadcast the `determine_username_format` \nfunction).\n\n```julia\nfunction get_emails(first_names::Vector{String}, last_names::Vector{String})\n    n = size(first_names, 1)\n    @assert n == size(last_names, 1) \"first and last name vectors must be same size\"\n\n    formats = determine_username_format.(first_names, last_names)\n    domains = determine_email_domains(n)\n\n    return formats .* domains\nend\n```\n\nYou'll notice that this is a deviation from the original post in that people's names and their \nemail addresses are now consistent since their names were used to generate their emails. This \nis an enhancement that is partly born out of convenience (we're already generating first and last \nnames, why spend more time generating more?) and partly because if we're getting our hands \ndirty with hand-tuned optimization, we might as well make the dataset slightly more realistic. \nSo we lose some (only three domains) and win some (email addresses based off the names).\n\nI haven't tested whether the closures are more helpful here, instead of regular functions, \nbut I'm sort of keeping them as is mainly to keep the code somewhat organized. They also don't \nneed to start with an `_` but, again, helps me visually parse them a little easier.\n\n\n#### company\n\nSimilar to the email functions above, I use closures to hold the pattern for the various \ncompany name formats, and then randomly select one of them to generate the pattern. Also, the \n`get_last_names` function gets re-used (yay).\n\n```julia\nfunction get_companies(n::Int)\n    suffixes = (\" Inc\", \" and Sons\", \" and Daughters\", \", LLC\", \" Group\")\n\n    _last_suffix(ln1, ln2, ln3) = ln1 * rand(suffixes)\n    _last_last(ln1, ln2, ln3) = ln1 * \"-\" * ln2\n    _last_x3(ln1, ln2, ln3) = ln1 * \", \" * ln2 * \" and \" * ln3\n    formats = (_last_suffix, _last_last, _last_x3)\n\n    last_names1 = get_last_names(n)\n    last_names2 = get_last_names(n)\n    last_names3 = get_last_names(n)\n\n    _company_name(ln1, ln2, ln3) = rand(formats)(ln1, ln2, ln3)\n\n    return _company_name.(last_names1, last_names2, last_names3)\nend\n```\n\nOnce again, the inner functions don't all use the given arguments, but we need that for the \nmini `_company_name` API.\n\n\n#### phone\n\nThis one was kept pretty much as is from the package, and that's fine because as we'll see its \nperformance is just fine as it is.\n\n```julia\nfunction get_phone_numbers(n::Int)\n    return [Faker.phone_number() for _ in 1:n]\nend\n```\n\n#### Save All the People\n\nPutting it all together, we get this:\n\n```julia\nfunction get_the_people(n::Int)\n    df = DataFrame(\n        id = get_id(n),\n        first_name = get_first_names(n),\n        last_name = get_last_names(n),\n        company= get_companies(n),\n        phone_number = get_phone_numbers(n),\n    )\n    df.email = get_emails(df.first_name, df.last_name)\n    return df\nend\n```\n\nAs you can see, we dispensed with the `Person` struct since we don't really need it. Instead, \nwe generate a `DataFrame` directly. And because we need our first and last names before we can \ngenerate our email addresses, the email field comes last.\n\nFinally, we have actual data generation and saving to parquet files.\n\n```julia\nfunction save_all_the_people(num_people::Int, num_files::Int)\n    for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/persons_$(file_num).parquet\"\n        df = get_the_people(num_people)\n        writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n```\n\nNow it's time to benchmark again.\n\n## The Final Benchmarks\n\nBefore we benchmark the big one above, we'll do some micro-benchmarking and test the \nfunctions that create each of the fields:\n\n```julia\n## benchmarks\nconst N = 1_000_000\n\n# get_emails depends on other fields being generated first, \n# which needs to be done outside of benchmarking\nfirst_names = get_first_names(N)\nlast_names = get_last_names(N)\n\n@btime get_id($N);\n@btime get_first_names($N);\n@btime get_last_names($N);\n@btime get_emails($first_names, $last_names);\n@btime get_companies($N);\n@btime get_phone_numbers($N);\n@btime get_the_people($N);\n```\n```\n  3.186 ms (2 allocations: 7.63 MiB)\n  4.270 ms (5 allocations: 7.67 MiB)\n  4.134 ms (3 allocations: 7.63 MiB)\n  309.268 ms (5747839 allocations: 262.10 MiB)\n  80.762 ms (2331987 allocations: 128.40 MiB)\n  367.853 ms (3000002 allocations: 187.66 MiB)\n  812.161 ms (11082578 allocations: 645.02 MiB)\n```\n\nLooks like we'll be spending the most time generating emails and phone numbers. We'll see if \nwe can cut that down eventually.\n\nTesting the big one:\n\n```julia\n@btime save_all_the_people(N, 10);\n```\n```\n  16.852 s (170856196 allocations: 10.43 GiB)\n```\n\nOk, not bad, generating 10 million people and saving to disk takes 17 seconds. But let's see \nif we can do better. We'll try multi-threading again, but this time just the `@threads` macro \nin front of the `for` loop should be good enough:\n\n```julia\nfunction save_all_the_people_threaded(num_people::Int, num_files::Int)\n    Threads.@threads for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/persons_$(file_num).parquet\"\n        df = get_the_people(num_people)\n        writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n```\n\nWe'll go for 16 threads right out of the gate:\n\n```julia\n@btime save_all_the_people_threaded(N, 10);\n```\n```\n  4.709 s (170854140 allocations: 10.43 GiB)\n```\n\nNot bad at all, though we're only getting a 3x speed-up. Let's try with 100 million:\n\n```julia\n@btime save_all_the_people_threaded(N, 100);\n```\n\nHere's a screenshot of `htop` while this is happening btw:\n![](htop.png)\n\nBenchmark result of 100 million people:\n\n```\n  94.074 s (1708559591 allocations: 104.34 GiB)\n```\n\nOk interesting, looks like we can't assume linear scaling after all! By that logic, 100 \nmillion should have taken about 50 seconds, but it actually took about twice that.\n\nNot to belabor the point, **doing an actual run with 1 billion people (1,000 parquet files) \ntook about 23 minutes!**\n\n\n## Concluding Thoughts\n\nTurns out we can indeed speed up the generation of fake data by quite a bit using plain Julia \nand a moderately powered laptop.\n\nI might explore further optimization opportunities and do some proper profiling in future \nposts. For completeness, it would behoove me to play around with multi-processing, which I \ndidn't here since it's more of a hassle and I doubt it would yield any appreciable performance \nimprovement over the threaded version. I suspect there are significant gains to be made just \nhand-tuning the serial code, especially the string handling in the `get_emails` and \n`get_phone_numbers` functions.\n\nI can imagine someone arguing, \"but you probably spent more time researching this issue and \nwriting custom code... surely $12 and 2 hours on an EC2 makes more sense for a one-time run \nlike this.\"\n\nAbsolutely. I definitely spent more than 2 hours looking into Faker.jl and coming up with \nmy own. And if we were just trying to get a job done, that would be the way to go. But... \nwhere's the fun in that?\n\nThis way, we learned a thing or two (or at least I did). And most importantly we are now armed \nwith enough knowledge to make a PR or two to the Faker.jl repo and have the whole Julia and \ndata communities benefit. If time allows, that is indeed what I'll be looking to do.\n\nLastly, I also hope that any newbie Julia programmers or various data practioners (data \nenginners, data scientists, data analysits) will benefit from reading this post and learn a \nthing or two themselves.\n\nCheers and happy coding :)\n\n\n## Appendix: Software and Hardware\n\nThe code in this post was run with Julia 1.8.5 and the following package versions:\n\n```julia\nusing Pkg\n\nPkg.status()\n```\n```\nStatus `~/Dev/Julia/FakePeople/Project.toml`\n  [a93c6f00] DataFrames v1.5.0\n  [0efc519c] Faker v0.3.5\n  [98572fba] Parquet2 v0.2.9\n  [ac1d9e8a] ThreadsX v0.1.11\n```\n\nwith the following hardware/software specs:\n\n```julia\nversioninfo()\n```\n```\nJulia Version 1.8.5\nCommit 17cfb8e65ea (2023-01-08 06:45 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 × AMD Ryzen 7 PRO 4750U with Radeon Graphics\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-13.0.1 (ORCJIT, znver2)\n  Threads: 12 on 16 virtual cores\nEnvironment:\n  JULIA_NUM_THREADS = 12\n  JULIA_EDITOR = code\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.217","theme":"simplex","title-block-banner":false,"title":"Generating 1 Billion Fake People with Julia","date":"2023-02-27"},"extensions":{"book":{"multiFile":true}}}}}
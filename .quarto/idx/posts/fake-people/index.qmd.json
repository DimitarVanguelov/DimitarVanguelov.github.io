{"title":"Generating Fake People with Julia","markdown":{"yaml":{"title":"Generating Fake People with Julia","date":"2023-02-22"},"headingText":"The Details","containsRefs":false,"markdown":"\n\nIn a [recent MotherDuck blog post](https://motherduck.com/blog/python-faker-duckdb-exploration/), \nthe author generated 1 billion fake people records using Python in order to analyze the data \nwith DuckDB. I suspect the point of the article was to showcase how awesome `duckdb` is at \nhandling large amounts of local data (I personally didn't need the extra convincing, I was \nalready a fan), but it did spend the majority of its time explaining the data generation process.\n\nOne of the most interesting tidbits from the article to me was (emphasis mine):\n\n> I used the GNU Parallel technique discussed above with a **hefty m6i.32xlarge instance** on \nAmazon EC2, though generated a billion people in 1k parquet files. This **took about 2 hours to \ngenerate**.\n\nYikes, that's a lot of firepower! In case you're [too lazy to look it up](https://instances.vantage.sh/aws/ec2/m6i.32xlarge), \nthat machine comes with 128 vCPUs and 512 GiB RAM, and costs about $6 an hour. So pretty hefty \nindeed.\n\nNow, being a Julia user -- there is something in us that just compels us to want to make things \ngo faster, and it's not clear if we're drawn to Julia because it's an innate quality in us, or \nif programming in Julia breaks our brains and makes us want to optimize... like everything -- \nI immediately nerd-sniped myself into seeing if I could speed up this fake generation of people \nrecords, not on some beefy instance, but on my plain old laptop.\n\nThe results were initially disappointing, as I'll explain below, but in the end I did get that \nnice speed-up I was looking for.\n\n\n\nRefer to the original post for the full details, but I'll go over the basic details here. A \n`person` record consists of the following randomly generated fields:\n```\n- id\n- first_name\n- last_name\n- email\n- company\n- phone\n```\n\nUsing the Python [Faker](https://faker.readthedocs.io/en/master/) library to generate the \ndata and [GNU Parallel](https://www.gnu.org/software/parallel/) to parallelize the operation, \nthe author created 1,000 parquet files with 1 million records each before populating a \n`duckdb` database for further analysis.\n\nIn this post, we'll explore Julia's own \n[Faker.jl](https://github.com/neomatrixcode/Faker.jl) package, and how to leverage the various, \nbuilt-in capabilities Julia has for concurrency and parallelism.\n\n\n## Julia: First Attempt\n\nAs mentioned, Julia has its own Faker library. Using it is as simple as:\n\n```{julia}\nusing Faker\n\nFaker.first_name()\n```\n\nInstead of putting all the fields in a dictionary, I created a struct instead:\n\n```{julia}\nstruct Person\n    id::String\n    first_name::String\n    last_name::String\n    email::String\n    company::String\n    phone::String\nend\n```\n\nAside from being a natural thing to do in Julia, this ended up being a really handy vehicle \nfor populating a DataFrame, as we'll see in a moment.\n\nIn order to construct the `Person` object, we have the following function, which is essentially \nthe same as in the Python version in the original post:\n\n```{julia}\nfunction get_person()\n    person = Person(\n        Faker.random_int(min=1000, max=9999999999999),\n        Faker.first_name(),\n        Faker.last_name(),\n        Faker.email(),\n        Faker.company(),\n        Faker.phone_number()\n    )\n    return person\nend\n\nget_person()\n```\n\nThis approach clearly suffers from the same deficiency as the original in that the generated \nemail address bears absolutely no semblance to the generated first and last names ðŸ˜‚. But \nthat's ok, we're just making up data for mocking and testing purposes anyhow.\n\nTo create an array of `Person`s, we can use a comprehension:\n\n```{julia}\nlist_of_five = [get_person() for _ in 1:5]\n```\n\nNotice how we get a `Vector` of `Person`s... this is partially where that cool thing happens. \nPlacing that vector in a `DataFrame` constructor creates a dataframe object for us without any \nhassle at all:\n\n```{julia}\n# | output: false\nusing DataFrames\n\ndf = DataFrame(list_of_five)\n```\n\n```\n5Ã—6 DataFrame\n Row â”‚ id             first_name  last_name  email                      company                         phone                 \n     â”‚ String         String      String     String                     String                          String                \nâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n   1 â”‚ 9006394878384  Treva       Friesen    Funk.Roy@ar.name           Boehm-Roberts                   624-581-8651 x27099\n   2 â”‚ 4911678414881  Demetra     Wiza       TBechtelar@hotmail.com     Fritsch-Ebert                   504.840.2004 x016\n   3 â”‚ 5665302809885  Felipa      Bradtke    Hermann.Maurice@gmail.com  Lockman, Wintheiser and Cronin  530.779.9959 x685\n   4 â”‚ 2166373345058  Janie       Berge      Carita38@hotmail.com       Shields-Schowalter              1-855-593-7414 x54562\n   5 â”‚ 5865747761410  Landon      McKenzie   Kihn.Lauri@was.co          Lueilwitz-Daniel                (296) 989-2137 x8766\n```\n\nThat's pretty neat!\n\nAnyhow, with our basic functionality all set up, it's time to do some light benchmarking to \nget a sense of how this code will perform. I started off small by generating only a 100,000 \nrecords:\n\n```{julia}\n@time [get_person() for _ in 1:100_000] |> DataFrame;\n```\n\nOof, that result is not very comforting -- taking a minute plus just for 100,000 records does \nnot bode well. Assuming linear scaling, it would take 65 * 10_000 seconds, or roughly 180 hours \nto run the full thing ðŸ˜°.\n\nAt this point, I'm thinking we can speed things up a bit by using multi-threading. But figuring \nout the right syntax for creating an array and then populating it with data using threading \nappeared a bit clunky. Luckily there exists the ThreadsX.jl package that allows us to use \ncomprehensions for such things, specifically by using `ThreadsX.collect` over our comprehension:\n\n```{julia}\nusing ThreadsX\n\n@time ThreadsX.collect(get_person() for _ in 1:100_000) |> DataFrame;\n```\n\nOk so that's a little better, but running 12 threads and getting a 5-6x speed-up is not that \ngreat, but, more importantly, by our linear scaling logic, the full 1 billion record run \nwould take approximately 30 hours on my laptop. Just to generate the data, nevermind \nserializing it to disk.\n\nDespite knowing it's a losing battle, I wrote a function to generate all the data and save it \nas parquet files, just like in the original post:\n\n```{julia}\nusing Parquet2: writefile\n\nfunction save_the_people(num_people, num_files)\n    @sync for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/outfile_$(file_num).parquet\"\n        df = ThreadsX.collect(get_person() for _ in 1:num_people) |> DataFrame\n        @async writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n```\n\n## Appendix\n\nThe code in this post was run with Julia 1.8.5 and the following package versions:\n\n```{julia}\nusing Pkg\n\nPkg.status([\"Faker\", \"DataFrames\", \"Parquet2\", \"ThreadsX\"])\n```\n\nAdditional hardware and software info:\n```{julia}\nversioninfo()\n```\n","srcMarkdownNoYaml":"\n\nIn a [recent MotherDuck blog post](https://motherduck.com/blog/python-faker-duckdb-exploration/), \nthe author generated 1 billion fake people records using Python in order to analyze the data \nwith DuckDB. I suspect the point of the article was to showcase how awesome `duckdb` is at \nhandling large amounts of local data (I personally didn't need the extra convincing, I was \nalready a fan), but it did spend the majority of its time explaining the data generation process.\n\nOne of the most interesting tidbits from the article to me was (emphasis mine):\n\n> I used the GNU Parallel technique discussed above with a **hefty m6i.32xlarge instance** on \nAmazon EC2, though generated a billion people in 1k parquet files. This **took about 2 hours to \ngenerate**.\n\nYikes, that's a lot of firepower! In case you're [too lazy to look it up](https://instances.vantage.sh/aws/ec2/m6i.32xlarge), \nthat machine comes with 128 vCPUs and 512 GiB RAM, and costs about $6 an hour. So pretty hefty \nindeed.\n\nNow, being a Julia user -- there is something in us that just compels us to want to make things \ngo faster, and it's not clear if we're drawn to Julia because it's an innate quality in us, or \nif programming in Julia breaks our brains and makes us want to optimize... like everything -- \nI immediately nerd-sniped myself into seeing if I could speed up this fake generation of people \nrecords, not on some beefy instance, but on my plain old laptop.\n\nThe results were initially disappointing, as I'll explain below, but in the end I did get that \nnice speed-up I was looking for.\n\n\n## The Details\n\nRefer to the original post for the full details, but I'll go over the basic details here. A \n`person` record consists of the following randomly generated fields:\n```\n- id\n- first_name\n- last_name\n- email\n- company\n- phone\n```\n\nUsing the Python [Faker](https://faker.readthedocs.io/en/master/) library to generate the \ndata and [GNU Parallel](https://www.gnu.org/software/parallel/) to parallelize the operation, \nthe author created 1,000 parquet files with 1 million records each before populating a \n`duckdb` database for further analysis.\n\nIn this post, we'll explore Julia's own \n[Faker.jl](https://github.com/neomatrixcode/Faker.jl) package, and how to leverage the various, \nbuilt-in capabilities Julia has for concurrency and parallelism.\n\n\n## Julia: First Attempt\n\nAs mentioned, Julia has its own Faker library. Using it is as simple as:\n\n```{julia}\nusing Faker\n\nFaker.first_name()\n```\n\nInstead of putting all the fields in a dictionary, I created a struct instead:\n\n```{julia}\nstruct Person\n    id::String\n    first_name::String\n    last_name::String\n    email::String\n    company::String\n    phone::String\nend\n```\n\nAside from being a natural thing to do in Julia, this ended up being a really handy vehicle \nfor populating a DataFrame, as we'll see in a moment.\n\nIn order to construct the `Person` object, we have the following function, which is essentially \nthe same as in the Python version in the original post:\n\n```{julia}\nfunction get_person()\n    person = Person(\n        Faker.random_int(min=1000, max=9999999999999),\n        Faker.first_name(),\n        Faker.last_name(),\n        Faker.email(),\n        Faker.company(),\n        Faker.phone_number()\n    )\n    return person\nend\n\nget_person()\n```\n\nThis approach clearly suffers from the same deficiency as the original in that the generated \nemail address bears absolutely no semblance to the generated first and last names ðŸ˜‚. But \nthat's ok, we're just making up data for mocking and testing purposes anyhow.\n\nTo create an array of `Person`s, we can use a comprehension:\n\n```{julia}\nlist_of_five = [get_person() for _ in 1:5]\n```\n\nNotice how we get a `Vector` of `Person`s... this is partially where that cool thing happens. \nPlacing that vector in a `DataFrame` constructor creates a dataframe object for us without any \nhassle at all:\n\n```{julia}\n# | output: false\nusing DataFrames\n\ndf = DataFrame(list_of_five)\n```\n\n```\n5Ã—6 DataFrame\n Row â”‚ id             first_name  last_name  email                      company                         phone                 \n     â”‚ String         String      String     String                     String                          String                \nâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n   1 â”‚ 9006394878384  Treva       Friesen    Funk.Roy@ar.name           Boehm-Roberts                   624-581-8651 x27099\n   2 â”‚ 4911678414881  Demetra     Wiza       TBechtelar@hotmail.com     Fritsch-Ebert                   504.840.2004 x016\n   3 â”‚ 5665302809885  Felipa      Bradtke    Hermann.Maurice@gmail.com  Lockman, Wintheiser and Cronin  530.779.9959 x685\n   4 â”‚ 2166373345058  Janie       Berge      Carita38@hotmail.com       Shields-Schowalter              1-855-593-7414 x54562\n   5 â”‚ 5865747761410  Landon      McKenzie   Kihn.Lauri@was.co          Lueilwitz-Daniel                (296) 989-2137 x8766\n```\n\nThat's pretty neat!\n\nAnyhow, with our basic functionality all set up, it's time to do some light benchmarking to \nget a sense of how this code will perform. I started off small by generating only a 100,000 \nrecords:\n\n```{julia}\n@time [get_person() for _ in 1:100_000] |> DataFrame;\n```\n\nOof, that result is not very comforting -- taking a minute plus just for 100,000 records does \nnot bode well. Assuming linear scaling, it would take 65 * 10_000 seconds, or roughly 180 hours \nto run the full thing ðŸ˜°.\n\nAt this point, I'm thinking we can speed things up a bit by using multi-threading. But figuring \nout the right syntax for creating an array and then populating it with data using threading \nappeared a bit clunky. Luckily there exists the ThreadsX.jl package that allows us to use \ncomprehensions for such things, specifically by using `ThreadsX.collect` over our comprehension:\n\n```{julia}\nusing ThreadsX\n\n@time ThreadsX.collect(get_person() for _ in 1:100_000) |> DataFrame;\n```\n\nOk so that's a little better, but running 12 threads and getting a 5-6x speed-up is not that \ngreat, but, more importantly, by our linear scaling logic, the full 1 billion record run \nwould take approximately 30 hours on my laptop. Just to generate the data, nevermind \nserializing it to disk.\n\nDespite knowing it's a losing battle, I wrote a function to generate all the data and save it \nas parquet files, just like in the original post:\n\n```{julia}\nusing Parquet2: writefile\n\nfunction save_the_people(num_people, num_files)\n    @sync for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/outfile_$(file_num).parquet\"\n        df = ThreadsX.collect(get_person() for _ in 1:num_people) |> DataFrame\n        @async writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n```\n\n## Appendix\n\nThe code in this post was run with Julia 1.8.5 and the following package versions:\n\n```{julia}\nusing Pkg\n\nPkg.status([\"Faker\", \"DataFrames\", \"Parquet2\", \"ThreadsX\"])\n```\n\nAdditional hardware and software info:\n```{julia}\nversioninfo()\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.217","theme":"simplex","title-block-banner":false,"title":"Generating Fake People with Julia","date":"2023-02-22"},"extensions":{"book":{"multiFile":true}}}}}
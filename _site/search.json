[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\nfunction foo(n)\n    return [2^i for i in 0:n]\nend\n\nfoo(10)\n\n11-element Vector{Int64}:\n    1\n    2\n    4\n    8\n   16\n   32\n   64\n  128\n  256\n  512\n 1024"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Iâ€™m just another dude blogging about programming on the internet, a data engineer who has a great fondness for Julia."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "backseat_programmer",
    "section": "",
    "text": "Generating Fake People with Julia\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fake-people/index.html",
    "href": "posts/fake-people/index.html",
    "title": "Generating Fake People with Julia",
    "section": "",
    "text": "In a recent MotherDuck blog post, the author generated 1 billion fake people records using Python in order to analyze the data with DuckDB. I suspect the point of the article was to showcase how awesome duckdb is at handling large amounts of local data (I personally didnâ€™t need the extra convincing, I was already a fan), but it did spend the majority of its time explaining the data generation process.\nOne of the most interesting tidbits from the article to me was (emphasis mine):\nYikes, thatâ€™s a lot of firepower! In case youâ€™re too lazy to look it up, that machine comes with 128 vCPUs and 512 GiB RAM, and costs about $6 an hour. So pretty hefty indeed.\nNow, being a Julia user â€“ there is something in us that just compels us to want to make things go faster, and itâ€™s not clear if weâ€™re drawn to Julia because itâ€™s an innate quality in us, or if programming in Julia breaks our brains and makes us want to optimizeâ€¦ like everything â€“ I immediately nerd-sniped myself into seeing if I could speed up this fake generation of people records, not on some beefy instance, but on my plain old laptop.\nThe results were initially disappointing, as Iâ€™ll explain below, but in the end I did get that nice speed-up I was looking for."
  },
  {
    "objectID": "posts/fake-people/index.html#the-details",
    "href": "posts/fake-people/index.html#the-details",
    "title": "Generating Fake People with Julia",
    "section": "The Details",
    "text": "The Details\nRefer to the original post for the full details, but Iâ€™ll go over the basic details here. A person record consists of the following randomly generated fields:\n- id\n- first_name\n- last_name\n- email\n- company\n- phone\nUsing the Python Faker library to generate the data and GNU Parallel to parallelize the operation, the author created 1,000 parquet files with 1 million records each before populating a duckdb database for further analysis.\nIn this post, weâ€™ll explore Juliaâ€™s own Faker.jl package, and how to leverage the various, built-in capabilities Julia has for concurrency and parallelism."
  },
  {
    "objectID": "posts/fake-people/index.html#julia-first-attempt",
    "href": "posts/fake-people/index.html#julia-first-attempt",
    "title": "Generating Fake People with Julia",
    "section": "Julia: First Attempt",
    "text": "Julia: First Attempt\nAs mentioned, Julia has its own Faker library. Using it is as simple as:\n\nusing Faker\n\nFaker.first_name()\n\n\"Heath\"\n\n\nInstead of putting all the fields in a dictionary, I created a struct instead:\n\nstruct Person\n    id::String\n    first_name::String\n    last_name::String\n    email::String\n    company::String\n    phone::String\nend\n\nAside from being a natural thing to do in Julia, this ended up being a really handy vehicle for populating a DataFrame, as weâ€™ll see in a moment.\nIn order to construct the Person object, we have the following function, which is essentially the same as in the Python version in the original post:\n\nfunction get_person()\n    person = Person(\n        Faker.random_int(min=1000, max=9999999999999),\n        Faker.first_name(),\n        Faker.last_name(),\n        Faker.email(),\n        Faker.company(),\n        Faker.phone_number()\n    )\n    return person\nend\n\nget_person()\n\nPerson(\"531318646699\", \"Pearline\", \"Roberts\", \"Hettinger.Tory@bl.org\", \"Wiegand LLC\", \"107-936-9117 x69192\")\n\n\nThis approach clearly suffers from the same deficiency as the original in that the generated email address bears absolutely no semblance to the generated first and last names ðŸ˜‚. But thatâ€™s ok, weâ€™re just making up data for mocking and testing purposes anyhow.\nTo create an array of Persons, we can use a comprehension:\n\nlist_of_five = [get_person() for _ in 1:5]\n\n5-element Vector{Person}:\n Person(\"7458738230968\", \"Carina\", \"Bergnaum\", \"Tilda.Hettinger@hgar.biz\", \"Ortiz-D'Amore\", \"310.246.4443\")\n Person(\"7458344071827\", \"Paul\", \"Sporer\", \"Eliz.Stoltenberg@hotmail.com\", \"Bins and Sons\", \"1-795-159-6632\")\n Person(\"2925283190195\", \"Gabriel\", \"Stoltenberg\", \"Mamie78@hotmail.com\", \"Funk-Jaskolski\", \"827-315-2192\")\n Person(\"7964768739855\", \"Theresia\", \"Raynor\", \"pReinger@yahoo.com\", \"Hayes, Murazik and Huels\", \"897-004-9036 x61245\")\n Person(\"8650989609058\", \"Jed\", \"Lakin\", \"Enoch72@sbag.co\", \"Dare-Braun\", \"(646) 706-8481 x3264\")\n\n\nNotice how we get a Vector of Personsâ€¦ this is partially where that cool thing happens. Placing that vector in a DataFrame constructor creates a dataframe object for us without any hassle at all:\n\nusing DataFrames\n\ndf = DataFrame(list_of_five)\n\n5Ã—6 DataFrame\n Row â”‚ id             first_name  last_name  email                      company                         phone                 \n     â”‚ String         String      String     String                     String                          String                \nâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n   1 â”‚ 9006394878384  Treva       Friesen    Funk.Roy@ar.name           Boehm-Roberts                   624-581-8651 x27099\n   2 â”‚ 4911678414881  Demetra     Wiza       TBechtelar@hotmail.com     Fritsch-Ebert                   504.840.2004 x016\n   3 â”‚ 5665302809885  Felipa      Bradtke    Hermann.Maurice@gmail.com  Lockman, Wintheiser and Cronin  530.779.9959 x685\n   4 â”‚ 2166373345058  Janie       Berge      Carita38@hotmail.com       Shields-Schowalter              1-855-593-7414 x54562\n   5 â”‚ 5865747761410  Landon      McKenzie   Kihn.Lauri@was.co          Lueilwitz-Daniel                (296) 989-2137 x8766\nThatâ€™s pretty neat!\nAnyhow, with our basic functionality all set up, itâ€™s time to do some light benchmarking to get a sense of how this code will perform. I started off small by generating only a 100,000 records:\n\n@time [get_person() for _ in 1:100_000] |&gt; DataFrame;\n\n 65.911398 seconds (51.68 M allocations: 2.484 GiB, 1.61% gc time, 0.15% compilation time: 30% of which was recompilation)\n\n\nOof, that result is not very comforting â€“ taking a minute plus just for 100,000 records does not bode well. Assuming linear scaling, it would take 65 * 10_000 seconds, or roughly 180 hours to run the full thing ðŸ˜°.\nAt this point, Iâ€™m thinking we can speed things up a bit by using multi-threading. But figuring out the right syntax for creating an array and then populating it with data using threading appeared a bit clunky. Luckily there exists the ThreadsX.jl package that allows us to use comprehensions for such things, specifically by using ThreadsX.collect over our comprehension:\n\nusing ThreadsX\n\n@time ThreadsX.collect(get_person() for _ in 1:100_000) |&gt; DataFrame;\n\n 11.608021 seconds (54.56 M allocations: 2.674 GiB, 6.03% gc time, 6.52% compilation time)\n\n\nOk so thatâ€™s a little better, but running 12 threads and getting a 5-6x speed-up is not that great, but, more importantly, by our linear scaling logic, the full 1 billion record run would take approximately 30 hours on my laptop. Just to generate the data, nevermind serializing it to disk.\nDespite knowing itâ€™s a losing battle, I wrote a function to generate all the data and save it as parquet files, just like in the original post:\n\nusing Parquet2: writefile\n\nfunction save_the_people(num_people, num_files)\n    @sync for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/outfile_$(file_num).parquet\"\n        df = ThreadsX.collect(get_person() for _ in 1:num_people) |&gt; DataFrame\n        @async writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\n\nsave_the_people (generic function with 1 method)"
  },
  {
    "objectID": "posts/fake-people/index.html#appendix",
    "href": "posts/fake-people/index.html#appendix",
    "title": "Generating Fake People with Julia",
    "section": "Appendix",
    "text": "Appendix\nThe code in this post was run with Julia 1.8.5 and the following package versions:\n\nusing Pkg\n\nPkg.status([\"Faker\", \"DataFrames\", \"Parquet2\", \"ThreadsX\"])\n\nStatus `~/.julia/environments/v1.8/Project.toml`\n  [a93c6f00] DataFrames v1.5.0\n  [0efc519c] Faker v0.3.5\n  [98572fba] Parquet2 v0.2.9\n  [ac1d9e8a] ThreadsX v0.1.11\n\n\nAdditional hardware and software info:\n\nversioninfo()\n\nJulia Version 1.8.5\nCommit 17cfb8e65ea (2023-01-08 06:45 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 Ã— AMD Ryzen 7 PRO 4750U with Radeon Graphics\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-13.0.1 (ORCJIT, znver2)\n  Threads: 16 on 16 virtual cores\nEnvironment:\n  JULIA_EDITOR = code\n  JULIA_NUM_THREADS = auto"
  }
]
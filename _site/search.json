[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\nfunction foo(n)\n    return [2^i for i in 0:n]\nend\n\nfoo(10)\n\n11-element Vector{Int64}:\n    1\n    2\n    4\n    8\n   16\n   32\n   64\n  128\n  256\n  512\n 1024"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m just another dude blogging about programming on the internet, a data engineer who has a great fondness for Julia."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "backseat_programmer",
    "section": "",
    "text": "Generating 1 Billion Fake People with Julia\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fake-people/index.html",
    "href": "posts/fake-people/index.html",
    "title": "Generating 1 Billion Fake People with Julia",
    "section": "",
    "text": "In a recent MotherDuck blog post, the author generated 1 billion fake people records using Python in order to analyze the data with DuckDB. I suspect the point of the article was to showcase how awesome duckdb is at handling large amounts of local data, but it did spend the majority of its time explaining the data generation process, which made for a fun read.\nOne of the more interesting tidbits from the article was:\nYikes, that’s a lot of firepower! That machine comes with 128 vCPUs and 512 GiB RAM, and costs about $6 an hour. So pretty hefty indeed.\nBeing a big fan of Julia, I decided to see what it would be like to generate such a dataset with Julia. More concretely, I wanted to see if I can use less resources (only my laptop) and have the process run in significantly less time. Not only that, I thought it be fun to explore the various modes of concurrency (multi-threading, multi-processing, async, etc) Julia offers.\nThe results were initially disappointing, as I’ll explain below, but in the end I did get that nice speed-up I was looking for."
  },
  {
    "objectID": "posts/fake-people/index.html#the-details",
    "href": "posts/fake-people/index.html#the-details",
    "title": "Generating 1 Billion Fake People with Julia",
    "section": "The Details",
    "text": "The Details\nThe original post has the full details, but I’ll go over the basic details here. A person record consists of the following randomly generated fields:\n- id\n- first_name\n- last_name\n- email\n- company\n- phone\nUsing the Python Faker library to generate the data and GNU Parallel to parallelize the operation, the author created 1,000 parquet files with 1 million records each before populating a duckdb database for further analysis."
  },
  {
    "objectID": "posts/fake-people/index.html#julia-first-attempt",
    "href": "posts/fake-people/index.html#julia-first-attempt",
    "title": "Generating 1 Billion Fake People with Julia",
    "section": "Julia: First Attempt",
    "text": "Julia: First Attempt\nLuckily, Julia has its own Faker.jl package. Using it is as simple as:\nusing Faker\n\nFaker.first_name()\n\"Wilfredo\"\nInstead of putting all the fields in a dictionary, I created a struct instead:\nstruct Person\n    id::String\n    first_name::String\n    last_name::String\n    email::String\n    company::String\n    phone::String\nend\nAside from being a natural thing to do in Julia, this ended up being a really handy vehicle for populating a DataFrame, as we’ll see in a moment.\nIn order to construct the Person object, we have the following function, which is essentially the same as in the Python version in the original post:\nfunction get_person()\n    person = Person(\n        Faker.random_int(min=1000, max=9999999999999),\n        Faker.first_name(),\n        Faker.last_name(),\n        Faker.email(),\n        Faker.company(),\n        Faker.phone_number()\n    )\n    return person\nend\n\nget_person()\nPerson(\"8429894898777\", \"Christin\", \"Gleason\", \"Archie99@yahoo.com\", \"Mante, Hilll and Hessel\", \"1-548-869-5799 x26945\")\nThis approach clearly suffers from the same deficiency as the original in that the generated email address bears absolutely no semblance to the generated first and last names. But that’s ok, we’re just making up data for mocking and testing purposes anyhow.\nTo create an array of Persons, we can use a comprehension:\nlist_of_five = [get_person() for _ in 1:5]\n5-element Vector{Person}:\n Person(\"502327436522\", \"Simon\", \"Lind\", \"Franklyn.Satterfield@yahoo.com\", \"Rutherford-Barton\", \"054.718.0236\")\n Person(\"1988647737198\", \"Charlott\", \"Jacobs\", \"Walter.Ziemann@hotmail.com\", \"Towne, Gorczany and Brekke\", \"839-605-0245 x477\")\n Person(\"3335059941285\", \"Glory\", \"Brakus\", \"Nienow.Cassandra@lh.net\", \"Schuppe, Powlowski and Powlowski\", \"(122) 872-3081 x3643\")\n Person(\"4996530776723\", \"Hedwig\", \"Pfannerstill\", \"wSchamberger@hg.net\", \"Langosh Group\", \"(594) 274-0196 x72486\")\n Person(\"2217875886672\", \"Coletta\", \"Effertz\", \"Whitley.Bechtelar@mz.org\", \"Rippin Inc\", \"991.601.1323\")\nNotice how we get a Vector of Persons… this is partially where that cool thing happens. Placing that vector in a DataFrame constructor creates a dataframe object for us without any hassle at all:\nusing DataFrames\n\ndf = DataFrame(list_of_five)\n5×6 DataFrame\n Row │ id             first_name  last_name     email                           company                           phone                 \n     │ String         String      String        String                          String                            String                \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ 502327436522   Simon       Lind          Franklyn.Satterfield@yahoo.com  Rutherford-Barton                 054.718.0236\n   2 │ 1988647737198  Charlott    Jacobs        Walter.Ziemann@hotmail.com      Towne, Gorczany and Brekke        839-605-0245 x477\n   3 │ 3335059941285  Glory       Brakus        Nienow.Cassandra@lh.net         Schuppe, Powlowski and Powlowski  (122) 872-3081 x3643\n   4 │ 4996530776723  Hedwig      Pfannerstill  wSchamberger@hg.net             Langosh Group                     (594) 274-0196 x72486\n   5 │ 2217875886672  Coletta     Effertz       Whitley.Bechtelar@mz.org        Rippin Inc                        991.601.1323\nThat’s pretty neat!\nAnyhow, with our basic functionality all set up, it’s time to do some light benchmarking to get a sense of how this code will perform. I started off small by generating only 10,000 records:\nusing BenchmarkTools\n\n@btime [get_person() for _ in 1:10_000] |&gt; DataFrame;\n6.531 s (5158878 allocations: 253.88 MiB)\nOof, that result is not very comforting – taking 6.5 seconds just for 10,000 records does not bode well. Assuming linear scaling (which we probably can’t), it would take 6.5 * 100,000 seconds, or roughly 180 hours, to run the full thing.\nAt this point, I’m already thinking this whole idea is dead in the water, but was curious if a solution existed and decided not to give up yet and keep exploring options.\n\nMulti-threading\nThe first tool in the arsenal to reach for in such a situation is probabably is multi-threading. Actually, the first tool to reach for is proper profiling, but when it comes to parallelizing code that is not ours, I would try multi-threading before exploring multi-processing or async.\nJulia has a really handy macro Threads.@threads that you can stick in front of a for loop to parallelize it. But using Base Julia to handcraft the threaded population of a vector is a bit clunky, in my opinion, thus I opted for the excellent ThreadsX.jl package, which makes this particular task a breeze. In essence, it parallelizes certain Base functions (such as sum, reduce, map, collect, etc.) and all one needs to do is simply put ThreadsX. in front of the function. In our case, it looks like this (with the benchmark result below):\nusing ThreadsX\n\n@btime ThreadsX.collect(get_person() for _ in 1:100_000) |&gt; DataFrame;\n1.072 s (5140629 allocations: 258.32 MiB)\nOk so that’s a little better, but running 12 threads and getting a 6x speed-up is not that great. More importantly, by our assumed linear scaling logic, the full 1 billion record run would still take approximately 30 hours on my laptop, just to generate the data, nevermind serializing it to disk.\n\n\nA quick benchmarking aside\nMy laptop has 8 physical / 16 logical cores. For some reason I set the default number of threads to 12… I honestly don’t know why. Perhaps I was thinking “let’s leave some for the others” ¯\\_(ツ)_/¯.\nNow it’s true that a lot of workloads won’t take advantage of all logical threads that are supposedly available, thus one is essentially bound to the number of physical cores, but some workloads might take advantage of those extra threads. So to be thorough, I ran the following benchmarks with 1 through 16 cores.\nI saved my code to a file named fake_naive.jl:\nusing BenchmarkTools\n\n# --snip--\n\nprint(Threads.nthreads(), \"\\t\\t\")\n@btime ThreadsX.collect(get_person() for _ in 1:10_000) |&gt; DataFrame;\nand a short bash script as follows:\n#!/bin/bash\n\nprintf 'Num Threads\\tBenchmark Results\\n'\n\nfor i in $(seq 1 16)\ndo\n    # start a julia process with i threads\n    julia -t \"$i\" -e 'include(\"fake_naive.jl\")'\ndone\nRunning it gave the following results:\nNum Threads     Benchmark Results\n1                 6.422 s (5146142 allocations: 256.55 MiB)\n2                 3.361 s (5146774 allocations: 256.92 MiB)\n3                 2.443 s (5141482 allocations: 257.37 MiB)\n4                 1.990 s (5149584 allocations: 257.76 MiB)\n5                 1.593 s (5138217 allocations: 257.54 MiB)\n6                 1.448 s (5140074 allocations: 257.62 MiB)\n7                 1.339 s (5141603 allocations: 257.70 MiB)\n8                 1.296 s (5141621 allocations: 257.69 MiB)\n9                 1.145 s (5139926 allocations: 258.29 MiB)\n10                1.112 s (5137139 allocations: 258.15 MiB)\n11                1.087 s (5146978 allocations: 258.64 MiB)\n12                1.050 s (5142290 allocations: 258.40 MiB)\n13                1.005 s (5137553 allocations: 258.16 MiB)\n14                966.497 ms (5139312 allocations: 258.26 MiB)\n15                955.672 ms (5147540 allocations: 258.66 MiB)\n16                906.340 ms (5140219 allocations: 258.30 MiB)\nwith the plotted version:\n\nThis is interesting. Clearly we see diminishing returns as we increase the number of threads past 8 or so, nevertheless there is a 30% speed improvement going from 8 threads to 16 threads, at least according to this benchmark.\nOnce again, assuming linear scaling, we can expect 16 threads to run in 0.90634 / 6.422 * 180 = 25.5 hours. Not nearly good enough!\n\n\nFinishing the naive code\nDespite knowing that my current approach is a losing battle, I wrote a function that generates the data and saves it to parquet files, in order to test out IO as well:\nusing Parquet2: writefile\n\nfunction save_the_people_sync(num_people, num_files)\n    for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/outfile_$(file_num).parquet\"\n        df = ThreadsX.collect(get_person() for _ in 1:num_people) |&gt; DataFrame\n        writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\nalong with a second async version:\nfunction save_the_people_async(num_people, num_files)\n    @sync for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/outfile_$(file_num).parquet\"\n        df = ThreadsX.collect(get_person() for _ in 1:num_people) |&gt; DataFrame\n        @async writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\nThe idea behind the @async code above is that while datasetn+1 is being generated, datasetn is being written to disk. At least in theory.\nBenchmarking the 10K records with 10 file writes (using 16 threads this time)\n@btime save_the_people_sync(10_000, 10);\n@btime save_the_people_async(10_000, 10);\ngave the following results:\n  11.598 s (52145292 allocations: 2.57 GiB)\n  11.018 s (52179590 allocations: 2.57 GiB)\nSo the async stuff doesn’t really have much an effect, at least for 10K records. Trying it out with 100K records results in:\n  109.919 s (521194490 allocations: 25.58 GiB)\n  111.094 s (521135072 allocations: 25.58 GiB)\nHmm… attempting to write bigger data asynchronously while using 16 threads to generate it actually resulted in a slight slowdown.\nI tried one more thing, because I was maybe over-subscribing my CPU – I bumped my threads down to 12 again and got 124.67s for the non-async and 123.13s for the async version. Clearly the async wasn’t helping much. Though we did get some confirmation that, for this use case, it makes sense to max out the number of available threads, seeing as how lowering the thread-count did result in a ~12 second slowdown.\nIn any case, it was time to move on."
  },
  {
    "objectID": "posts/fake-people/index.html#appendix",
    "href": "posts/fake-people/index.html#appendix",
    "title": "Generating 1 Billion Fake People with Julia",
    "section": "Appendix",
    "text": "Appendix\nThe code in this post was run with Julia 1.8.5 and the following package versions:\nusing Pkg\n\nPkg.status()\nStatus `~/Dev/Julia/FakePeople/Project.toml`\n  [a93c6f00] DataFrames v1.5.0\n  [0efc519c] Faker v0.3.5\n  [98572fba] Parquet2 v0.2.9\n  [ac1d9e8a] ThreadsX v0.1.11\nwith the following hardware:\nversioninfo()\nJulia Version 1.8.5\nCommit 17cfb8e65ea (2023-01-08 06:45 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 × AMD Ryzen 7 PRO 4750U with Radeon Graphics\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-13.0.1 (ORCJIT, znver2)\n  Threads: 12 on 16 virtual cores\nEnvironment:\n  JULIA_NUM_THREADS = 12\n  JULIA_EDITOR = code"
  },
  {
    "objectID": "posts/fake-people/index.html#appendix-software-and-hardware",
    "href": "posts/fake-people/index.html#appendix-software-and-hardware",
    "title": "Generating 1 Billion Fake People with Julia",
    "section": "Appendix: Software and Hardware",
    "text": "Appendix: Software and Hardware\nThe code in this post was run with Julia 1.8.5 and the following package versions:\nusing Pkg\n\nPkg.status()\nStatus `~/Dev/Julia/FakePeople/Project.toml`\n  [a93c6f00] DataFrames v1.5.0\n  [0efc519c] Faker v0.3.5\n  [98572fba] Parquet2 v0.2.9\n  [ac1d9e8a] ThreadsX v0.1.11\nwith the following hardware/software specs:\nversioninfo()\nJulia Version 1.8.5\nCommit 17cfb8e65ea (2023-01-08 06:45 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 × AMD Ryzen 7 PRO 4750U with Radeon Graphics\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-13.0.1 (ORCJIT, znver2)\n  Threads: 12 on 16 virtual cores\nEnvironment:\n  JULIA_NUM_THREADS = auto\n  JULIA_EDITOR = code"
  },
  {
    "objectID": "posts/fake-people/index.html#diving-into-fakers-internals",
    "href": "posts/fake-people/index.html#diving-into-fakers-internals",
    "title": "Generating 1 Billion Fake People with Julia",
    "section": "Diving into Faker’s Internals",
    "text": "Diving into Faker’s Internals\nClearly doing a, more or less, 1-1 translation to Julia was not working. I posted my initial findings on the Julia Slack, and one user’s response inspired my next step:\n\n… I guess that the runtime is dominated by Faker, is it optimized for massive fake generation at all?\n\nI started looking into Faker.jl’s internals and noticed a couple of things: it was not exactly idiomatic Julia and the API did not explicitly support passing in an argument for number of records generated (at least that I could observe). In other words, you have to call code like this:\nlast_name = Faker.last_name()\nemail     = Faker.email()\ncompany   = Faker.company()\nand not like this:\nN = 1_000_000\n\nlast_names = Faker.last_name(N)\nemails     = Faker.email(N)\ncompanies  = Faker.company(N)\nMeaning with each function call, you can only produce one record at a time. If you wanted to generate 1 million records, you would have to do 1 million function calls. And the way you’d do that is with a loop or comprehension, as I did above. But these million function calls might be more expensive than necessary.\nThe first thing I noticed about Faker.jl is that the data that it samples from, such as actual first and last names, is stored as text in YAML files, which makes sense. And for items such as email addresses and phone numbers, it stores a kind of pattern which then gets translated by Julia functions into (randomly generated) text.\nFor instance, the name.yml file contains this:\nen:\n  faker:\n    name:\n      male_first_name: [Aaron, Abdul, Abe, Abel, Abraham...]\n      female_first_name: [Abbey, Abbie, Abby, Abigail, Ada...]\n      first_name:\n        - \"#{female_first_name}\"\n        - \"#{male_first_name}\"\n      last_name: [Abbott, Abernathy, Abshire, Adams...]\n      ...\nThe function definitions for first names are:\nmale_first_name()::String = executor(data[\"faker\"][\"name\"][\"male_first_name\"])\nfemale_first_name()::String = executor(data[\"faker\"][\"name\"][\"female_first_name\"])\n\nfunction first_name(genere::String=\"None\")::String\n    if(cmp(genere, \"M\") == 0)\n        return male_first_name()\n    elseif(cmp(genere, \"F\") == 0)\n        return female_first_name()\n end\nThe data variable above is a dictionary holding all the data and patterns. When the package is first loaded, data is initialized as a global variable that is an empty dictionary of type Dict{Any, Any} and is then populated with the contents of the YAML files. It being a non-constant global variable may have something to do with the performance issues, though I’m not sure that actually makes much of a difference here (contrary to one of the main rules of writing performant Julia: don’t use untyped global variables).\nIn any case, calling\nFaker.data[\"faker\"][\"name\"][\"female_first_name\"]\nsimply returns a vector of names:\n4272-element Vector{String}:\n \"Abbey\"\n \"Abbie\"\n \"Abby\"\n \"Abigail\"\n \"Ada\"\n ⋮\n \"Zoraida\"\n \"Zula\"\n \"Zulema\"\n \"Zulma\"\nand all the executor function does is pick a name at random (though that’s not all it does, not for other types of data):\nFaker.executor(Faker.data[\"faker\"][\"name\"][\"female_first_name\"])\n\"Estela\"\nChecking this function for type stability with @code_warntype shows that it’s not type stable, though again I’m not sure if that’s a problem here (it may be, but I’d have to do some more testing, which I haven’t had the time to do).\nAnyhow, this gave me an idea and I decided to run some more benchmarks."
  },
  {
    "objectID": "posts/fake-people/index.html#julia-second-attempt",
    "href": "posts/fake-people/index.html#julia-second-attempt",
    "title": "Generating 1 Billion Fake People with Julia",
    "section": "Julia: Second Attempt",
    "text": "Julia: Second Attempt\nI won’t go into many more details of the Faker.jl internals because a) I’m still getting to know the package, and b) I’m not sure they’re all that relevant or interesting for the purpose of this write-up (perhaps another blogpost).\nI’d like to take a moment though to say that I sincerely hope this post does not come off as harsh or unduly critical of Faker.jl and its author. For one, I admire anyone who spends their free time writing open-source software, and in the case of Faker.jl, it fills a gap in the Julia ecosystem that, to my knowledge, no one else has stepped up to fill. Moreover, it has very nice documentation, albeit simple (and appropriately so). And last but far from least, the author appears to be kind and friendly, which counts for a lot in my book.\nThe next step was to write my own functions for each of the Person fields and benchmark them against the package-provided equivalents.\n\nid\nThis is perhaps the simplest one, it’s just using rand to sample from a range. In the original Python version, there was no requirement to have the id field as a string (though if I was designing that database and using random integers, I would be sure to turn them into strings and left-pad with zeros).\nget_id(n::Int) = rand(1000:9999999999999, n)\n\n\nlast_name\nThis one is almost the same as the first_name function, only we don’t have to concatenate two vectors together.\nfunction get_last_names(n::Int)\n    last_names = Faker.data[\"faker\"][\"name\"][\"last_name\"]\n    return rand(last_names, n)\nend\n\n\nemail\nThis one required a little bit more work. Instead of relying on the internal package mechanism to generate fake email addresses, I rolled my own.\nIn the first function, closures provide each of the possible email formats. A closure is randomly selected, and the function arguments, first_name and last_name, are used to generate a username.\nfunction determine_username_format(first_name::String, last_name::String)\n    # create closure for each type of format\n    _first_last(first_name, last_name) = first_name * \".\" * last_name\n    _last_first(first_name, last_name) = last_name * \".\" * first_name\n    _first_num(first_name, last_name) = first_name * string(rand(1:99))\n    _flast(first_name, last_name) = first(first_name, 1) * last_name\n    # randomly sample each format and return, not just function, but result from function\n    formats = (_first_last, _last_first, _first_num, _flast)\n    _email_format(first_name, last_name) = rand(formats)(first_name, last_name)\n    return _email_format(first_name, last_name) |&gt; lowercase\nend\nNote that this function only works on scalar inputs and will be broadcast to vectors of first and last names later. Also, note how _first_num takes two arguments but only uses one – we need that to satisfy our mini _email_format API. If _first_num was defined with only the first_name argument, and that function was randomly selected for any given iteration, it would bomb. Thus we leave it with the same args as all the other inner functions. I know it feels hacky – there is probably a name for this technique – but it simplifies the code.\nMoving on, although there are more than three email domains (in the real world and in the Faker packages), I kept things simple and only used the main three that each account for ~17% of “market share”. I know this is cheating a little bit, but I honestly ran out of patience and figured this was good enough to get an idea of how fast this code can go.\ndetermine_email_domains(n::Int) = rand((\"@gmail.com\", \"@yahoo.com\", \"@hotmail.com\"), n)\nFinally, we take the vectors of names that are already generated and use them to call the two helper functions above (using the . notation to broadcast the determine_username_format function).\nfunction get_emails(first_names::Vector{String}, last_names::Vector{String})\n    n = size(first_names, 1)\n    @assert n == size(last_names, 1) \"first and last name vectors must be same size\"\n\n    formats = determine_username_format.(first_names, last_names)\n    domains = determine_email_domains(n)\n\n    return formats .* domains\nend\nYou’ll notice that this is a deviation from the original post in that people’s names and their email addresses are now consistent since their names were used to generate their emails. This is an enhancement that is partly born out of convenience (we’re already generate first and last names, why spend more time generating more?) and partly because if we’re getting our hands dirty with hand-tuned optimization, we might as well make the dataset slightly more realistic. So we lose some (only three domains) and win some (email addresses based off the names).\nI haven’t tested whether the closures are more helpful here, instead of regular functions, but I’m sort of keeping them as is mainly to keep the code somewhat organized. They also don’t need to start with an _ but, again, helps me visually parse them a little easier.\n\n\ncompany\nSimilar to the email functions above, I use closures to hold the pattern for the various company name formats, and then randomly select one of them to generate the pattern. Also, the get_last_names function gets re-used (yay).\nfunction get_companies(n::Int)\n    suffixes = (\" Inc\", \" and Sons\", \" and Daughters\", \", LLC\", \" Group\")\n\n    _last_suffix(ln1, ln2, ln3) = ln1 * rand(suffixes)\n    _last_last(ln1, ln2, ln3) = ln1 * \"-\" * ln2\n    _last_x3(ln1, ln2, ln3) = ln1 * \", \" * ln2 * \" and \" * ln3\n    formats = (_last_suffix, _last_last, _last_x3)\n\n    last_names1 = get_last_names(n)\n    last_names2 = get_last_names(n)\n    last_names3 = get_last_names(n)\n\n    _company_name(ln1, ln2, ln3) = rand(formats)(ln1, ln2, ln3)\n\n    return _company_name.(last_names1, last_names2, last_names3)\nend\nOnce again, the inner functions don’t all use the given arguments, but we need that for the mini _company_name API.\n\n\nphone\nThis one was kept pretty much as is from the package, and that’s fine because as we’ll see it’s performance is just fine as it is.\nfunction get_phone_numbers(n::Int)\n    return [Faker.phone_number() for _ in 1:n]\nend\n\n\nSave All the People\nPutting it all together, we get this:\nfunction get_the_people(n::Int)\n    df = DataFrame(\n        id = get_id(n),\n        first_name = get_first_names(n),\n        last_name = get_last_names(n),\n        company= get_companies(n),\n        phone_number = get_phone_numbers(n),\n    )\n    df.email = get_emails(df.first_name, df.last_name)\n    return df\nend\nAs you can see, we dispensed with the Person struct since we don’t really need it. Instead, we generate a DataFrame directly. And because we need our first and last names before we can generate our email addresses, the email field comes last.\nFinally, we have actual data generation and saving to parquet files.\nfunction save_all_the_people(num_people::Int, num_files::Int)\n    for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/persons_$(file_num).parquet\"\n        df = get_the_people(num_people)\n        writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\nNow it’s time to benchmark again."
  },
  {
    "objectID": "posts/fake-people/index.html#the-revelation",
    "href": "posts/fake-people/index.html#the-revelation",
    "title": "Generating 1 Billion Fake People with Julia",
    "section": "The Revelation",
    "text": "The Revelation\nTo isolate the impact of just Faker.first_name() being called a million times, I ran the following:\n@btime [Faker.first_name() for _ in 1:1_000_000];\n  75.036 s (58000002 allocations: 2.84 GiB)\nMy idea was to see if sampling from a vector of names using Julia’s rand function was perhaps faster. To do that, I had to concat the male and female first name vectors and then sample that new vector:\nfunction get_first_names(n::Int)\n    first_names_female = Faker.data[\"faker\"][\"name\"][\"female_first_name\"]\n    first_names_male = Faker.data[\"faker\"][\"name\"][\"male_first_name\"]\n    first_names_all = vcat(first_names_female, first_names_male)\n    return rand(first_names_all, n)\nend\nBenchmarking this function yielded quite the surprise:\n@btime first_name = get_first_names(1_000_000);\n  4.195 ms (5 allocations: 7.67 MiB)\nWow.\nFour milliseconds vs 75 seconds. That is an almost 18,000x speed-up! And notice the allocations and memory usage as well: 5 allocations vs 58 million, and ~8MiB vs 2.84GiB.\nThis was the magic sauce I had been looking for all along."
  },
  {
    "objectID": "posts/fake-people/index.html#the-final-benchmarks",
    "href": "posts/fake-people/index.html#the-final-benchmarks",
    "title": "Generating 1 Billion Fake People with Julia",
    "section": "The Final Benchmarks",
    "text": "The Final Benchmarks\nBefore we benchmark the big one above, we’ll do some micro-benchmarking and test the functions that create each of the fields:\n## benchmarks\nconst N = 1_000_000\n\n# get_emails depends on other fields being generated first, \n# which needs to be done outside of benchmarking\nfirst_names = get_first_names(N)\nlast_names = get_last_names(N)\n\n@btime get_id($N);\n@btime get_first_names($N);\n@btime get_last_names($N);\n@btime get_emails($first_names, $last_names);\n@btime get_companies($N);\n@btime get_phone_numbers($N);\n@btime get_the_people($N);\n  3.186 ms (2 allocations: 7.63 MiB)\n  4.270 ms (5 allocations: 7.67 MiB)\n  4.134 ms (3 allocations: 7.63 MiB)\n  309.268 ms (5747839 allocations: 262.10 MiB)\n  80.762 ms (2331987 allocations: 128.40 MiB)\n  367.853 ms (3000002 allocations: 187.66 MiB)\n  812.161 ms (11082578 allocations: 645.02 MiB)\nLooks like we’ll be spending the most time generating emails and phone numbers. We’ll see if we can cut that down eventually.\nTesting the big one:\n@btime save_all_the_people(N, 10);\n  16.852 s (170856196 allocations: 10.43 GiB)\nOk, not bad, generating 10 million people and saving to disk takes 17 seconds. But let’s see if we can do better. We’ll try multi-threading again, but this time just the @threads macro in front of the for loop should be good enough:\nfunction save_all_the_people_threaded(num_people::Int, num_files::Int)\n    Threads.@threads for i in 1:num_files\n        file_num = string(i, pad=ndigits(num_files))\n        file_loc = \"./data/persons_$(file_num).parquet\"\n        df = get_the_people(num_people)\n        writefile(file_loc, df; compression_codec=:snappy)\n    end\nend\nWe’ll go for 16 threads right out of the gate:\n@btime save_all_the_people_threaded(N, 10);\n  4.709 s (170854140 allocations: 10.43 GiB)\nNot bad at all, though we’re only getting a 3x speed-up. Let’s try with 100 million:\n@btime save_all_the_people_threaded(N, 100);\nHere’s a screenshot of htop white this is happening btw: \nBenchmark result of 100 million people:\n  94.074 s (1708559591 allocations: 104.34 GiB)\nOk interesting, looks like we can’t assume linear scaling after all! By that logic, 100 million should have taken about 50 seconds, but it actually took about twice that.\nNot the belabor the point, doing an actual run with 1 billion people (1,000 parquet files) took about 23 minutes!"
  },
  {
    "objectID": "posts/fake-people/index.html#concluding-thoughts",
    "href": "posts/fake-people/index.html#concluding-thoughts",
    "title": "Generating 1 Billion Fake People with Julia",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nTurns out we can indeed speed up the generation of fake data by quite a bit using plain Julia and a moderately powered laptop.\nI might explore further optimization opportunities and do some proper profiling in future posts. For completeness, it would behoove me to play around with multi-processing, which I didn’t here since it’s more of a hassle and I doubt it would yield any appreciate performance improvement over the threaded version. I suspect there are significant gains to be made just hand-tuning the serial code, especially the string handling in the get_emails and get_phone_numbers functions.\nI can imagine someone arguing, “but you probably spent more time researching this issue and writing custom code… surely $12 and 2 hours on an EC2 makes more sense for a one-time run like this.”\nAbsolutely. I definitely spent more than 2 hours looking into Faker.jl and coming up with my own. And if we were just trying to get a job done, that would be the way to go. But… where’s the fun in that?\nThis way, we learned a thing or two (or at least I did). And most importantly we are now armed with enough knowledge to make a PR or two to the Faker.jl repo and have the whole Julia and data communities benefit. If time allows, that is indeed what I’ll be looking to do.\nLastly, I also hope that any newbie Julia programmers or various data practioners (data enginners, data scientists, data analysits) will benefit from reading this post and learn a thing or two themselves.\nCheers and happy coding :)"
  }
]